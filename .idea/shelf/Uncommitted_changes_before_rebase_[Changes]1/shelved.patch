Index: pls.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport numpy.linalg as la\r\n\r\nfrom typing import Optional\r\n\r\nfrom base import nipals\r\n\r\n\r\nclass PLS:\r\n    \"\"\" Partial least squares. \"\"\"\r\n    def __init__(self):\r\n        self._T: Optional[np.ndarry] = None\r\n        self._P: Optional[np.ndarry] = None\r\n        self._W: Optional[np.ndarry] = None\r\n        self._C: Optional[np.ndarry] = None\r\n        self.coef: Optional[np.ndarry] = None\r\n\r\n    def fit(self, x, y, n_comp=None) -> None:\r\n        \"\"\"\r\n        Fit PLS model\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n by p, where n number\r\n            of samples/instances, p number of variables\r\n        y: np.ndarray\r\n            Dependent variable with size n by 1\r\n        n_comp: int\r\n            Number of components. Default is None, which indicates that\r\n            smaller number between n and p will be used.\r\n\r\n        Returns\r\n        -------\r\n        PLS object\r\n\r\n        \"\"\"\r\n        n, r = x.shape\r\n        # pre-allocation\r\n        T = np.empty((n, n_comp))\r\n        P = np.empty((r, n_comp))\r\n        W = np.empty((r, n_comp))\r\n        C = np.empty(n_comp)\r\n        # iterate through components\r\n        for nc in range(n_comp):\r\n            w, u, c, t = nipals(x, y)\r\n            # loadings\r\n            p = np.dot(t, x) / np.dot(t, t)\r\n            # update data matrix for next component\r\n            x -= t[:, np.newaxis] * p\r\n            y -= t * c\r\n            # save to matrix\r\n            T[:, nc] = t\r\n            P[:, nc] = p\r\n            W[:, nc] = w\r\n            C[nc] = c\r\n\r\n        # save results to matrix\r\n        self._T = T\r\n        self._P = P\r\n        self._W = W\r\n        self._C = C\r\n\r\n        # coefficients\r\n        # noinspection SpellCheckingInspection\r\n        coefs = np.empty((n_comp, r))\r\n        for nc in range(n_comp):\r\n            coefs[nc] = np.dot(np.dot(\r\n                W[:, :nc], la.inv(np.dot(P[:, :nc].T, W[:, :nc]))\r\n            ), C[:nc])\r\n        self.coef = coefs\r\n\r\n    def predict(self, x, n_component=None) -> np.ndarray:\r\n        \"\"\" Do prediction. \"\"\"\r\n        npc = self.coef.shape[1] - 1\r\n        if n_component is not None and n_component < npc:\r\n            npc = n_component - 1\r\n        coef = self.coef[npc]\r\n        return np.dot(x, coef)\r\n\r\n    @property\r\n    def scores_x(self):\r\n        \"\"\" Scores.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Scores\r\n\r\n        \"\"\"\r\n        return self._T\r\n\r\n    @property\r\n    def loadings_x(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            loadings\r\n\r\n        \"\"\"\r\n        return self._P\r\n\r\n    @property\r\n    def weights_y(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            y scores\r\n\r\n        \"\"\"\r\n        return self._C\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pls.py b/pls.py
--- a/pls.py	
+++ b/pls.py	
@@ -113,3 +113,15 @@
 
         """
         return self._C
+
+    @property
+    def weigths_x(self):
+        """
+
+        Returns
+        -------
+            np.ndarray
+                x weights
+
+        """
+        return self._W
Index: opls.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nOrthogonal Projection on Latent Structure (O-PLS)\r\n\"\"\"\r\nimport numpy as np\r\nfrom numpy import linalg as la\r\nfrom typing import Optional, Tuple\r\nfrom base import nipals\r\n\r\n\r\nclass OPLS:\r\n    \"\"\"\r\n    Orthogonal Projection on Latent Structure (O-PLS).\r\n    Methods\r\n    ----------\r\n    predictive_scores: np.ndarray\r\n        First predictive score.\r\n    predictive_loadings: np.ndarray\r\n        Predictive loadings.\r\n    weights_y: np.ndarray\r\n        y weights.\r\n    orthogonal_loadings: np.ndarray\r\n        Orthogonal loadings.\r\n    orthogonal_scores: np.ndarray\r\n        Orthogonal scores.\r\n    \"\"\"\r\n    def __init__(self):\r\n        \"\"\"\r\n        TODO:\r\n            1. add arg for specifying the method for performing PLS\r\n\r\n        \"\"\"\r\n        # orthogonal score matrix\r\n        self._Tortho: Optional[np.ndarray] = None\r\n        # orthogonal loadings\r\n        self._Portho: Optional[np.ndarray] = None\r\n        # loadings\r\n        self._Wortho: Optional[np.ndarray] = None\r\n        # covariate weights\r\n        self._w: Optional[np.ndarray] = None\r\n\r\n        # predictive scores\r\n        self._T: Optional[np.ndarray] = None\r\n        self._P: Optional[np.ndarray] = None\r\n        self._C: Optional[np.ndarray] = None\r\n        # coefficients\r\n        self.coef: Optional[np.ndarray] = None\r\n        # total number of components\r\n        self.npc: Optional[int] = None\r\n\r\n    def fit(self, x, y, n_comp=None):\r\n        \"\"\"\r\n        Fit PLS model.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        y: np.ndarray\r\n            Dependent matrix with size n samples by 1, or a vector\r\n        n_comp: int\r\n            Number of components, default is None, which indicates that\r\n            largest dimension which is smaller value between n and p\r\n            will be used.\r\n\r\n        Returns\r\n        -------\r\n        OPLS object\r\n\r\n        Reference\r\n        ---------\r\n        [1] Trygg J, Wold S. Projection on Latent Structure (OPLS).\r\n            J Chemometrics. 2002, 16, 119-128.\r\n        [2] Trygg J, Wold S. O2-PLS, a two-block (X-Y) latent variable\r\n            regression (LVR) method with a integral OSC filter.\r\n            J Chemometrics. 2003, 17, 53-64.\r\n\r\n        \"\"\"\r\n        n, p = x.shape\r\n        npc = min(n, p)\r\n        if n_comp is not None and n_comp < npc:\r\n            npc = n_comp\r\n\r\n        # initialization\r\n        Tortho = np.empty((n, npc))\r\n        Portho = np.empty((p, npc))\r\n        Wortho = np.empty((p, npc))\r\n        T, P, C = np.empty((n, npc)), np.empty((p, npc)), np.empty(npc)\r\n\r\n        # X-y variations\r\n        tw = np.dot(y, x) / np.dot(y, y)\r\n        tw /= la.norm(tw)\r\n        # predictive scores\r\n        tp = np.dot(x, tw)\r\n        # components\r\n        w, u, _, t = nipals(x, y)\r\n        p = np.dot(t, x) / np.dot(t, t)\r\n        for nc in range(npc):\r\n            # orthoganol weights\r\n            w_ortho = p - (np.dot(tw, p) * tw)\r\n            w_ortho /= la.norm(w_ortho)\r\n            # orthogonal scores\r\n            t_ortho = np.dot(x, w_ortho)\r\n            # orthogonal loadings\r\n            p_ortho = np.dot(t_ortho, x) / np.dot(t_ortho, t_ortho)\r\n            # update X to the residue matrix\r\n            x -= t_ortho[:, np.newaxis] * p_ortho\r\n            # save to matrix\r\n            Tortho[:, nc] = t_ortho\r\n            Portho[:, nc] = p_ortho\r\n            Wortho[:, nc] = w_ortho\r\n            # predictive scores\r\n            tp -= t_ortho * np.dot(p_ortho, tw)\r\n            T[:, nc] = tp\r\n            C[nc] = np.dot(y, tp) / np.dot(tp, tp)\r\n\r\n            # next component\r\n            w, u, _, t = nipals(x, y)\r\n            p = np.dot(t, x) / np.dot(t, t)\r\n            P[:, nc] = p\r\n\r\n        self._Tortho = Tortho\r\n        self._Portho = Portho\r\n        self._Wortho = Wortho\r\n        # covariate weights\r\n        self._w = tw\r\n\r\n        # coefficients and predictive scores\r\n        self._T = T\r\n        self._P = P\r\n        self._C = C\r\n        self.coef = tw * C[:, np.newaxis]\r\n\r\n        self.npc = npc\r\n\r\n    def predict(self, x, n_component=None, return_scores=False)\\\r\n            -> np.ndarray | Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"\r\n        Predict the new coming data matrix.\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        n_component: int | None\r\n            Number of components.\r\n        return_scores: bool\r\n            Whether the scores should be returned.\r\n\r\n        Returns\r\n        -------\r\n        y: np.ndarray\r\n            Predicted scores for classification.\r\n        score: np.ndarray\r\n            Predictive scores.\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        coef = self.coef[n_component - 1]\r\n\r\n        y = np.dot(x, coef)\r\n        if return_scores:\r\n            return y, np.dot(x, self._w)\r\n\r\n        return y\r\n\r\n    def correct(self, x, n_component=None, return_scores=False, dot=np.dot):\r\n        \"\"\"\r\n        Correction of X\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Data matrix with size n by c, where n is number of\r\n            samples, and c is number of variables\r\n        n_component: int | None\r\n            Number of components. If is None, the number of components\r\n            used in fitting the model is used. Default is None.\r\n        return_scores: bool\r\n            Return orthogonal scores. Default is False.\r\n\r\n        Returns\r\n        -------\r\n        xc: np.ndarray\r\n            Corrected data, with same matrix size with input X.\r\n        t: np.ndarray\r\n            Orthogonal score, n by n_component.\r\n\r\n        \"\"\"\r\n        # TODO: Check X type and dimension consistencies between X and\r\n        #       scores in model.\r\n        xc = x.copy()\r\n        if n_component is None:\r\n            n_component = self.npc\r\n\r\n        if xc.ndim == 1:\r\n            t = np.empty(n_component)\r\n            for nc in range(n_component):\r\n                t_ = dot(xc, self._Wortho[:, nc])\r\n                xc -= t_ * self._Portho[:, nc]\r\n                t[nc] = t_\r\n        else:\r\n            n, c = xc.shape\r\n            t = np.empty((n, n_component))\r\n            # scores\r\n            for nc in range(n_component):\r\n                t_ = dot(xc, self._Wortho[:, nc])\r\n                xc -= t_[:, np.newaxis] * self._Portho[:, nc]\r\n                t[:, nc] = t_\r\n\r\n        if return_scores:\r\n            return xc, t\r\n\r\n        return xc\r\n\r\n    def predictive_score(self, n_component=None):\r\n        \"\"\"\r\n        Parameters\r\n        ----------\r\n        n_component: int\r\n            The component number.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive score.\r\n\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        return self._T[:, n_component-1]\r\n\r\n    def ortho_score(self, n_component=None):\r\n        \"\"\"\r\n\r\n        Parameters\r\n        ----------\r\n        n_component: int\r\n            The component number.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first orthogonal score.\r\n\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        return self._Tortho[:, n_component-1]\r\n\r\n    @property\r\n    def predictive_scores(self):\r\n        \"\"\" Orthogonal loadings. \"\"\"\r\n        return self._T\r\n\r\n    @property\r\n    def predictive_loadings(self):\r\n        \"\"\" Predictive loadings. \"\"\"\r\n        return self._P\r\n\r\n    @property\r\n    def weights_y(self):\r\n        \"\"\" y scores. \"\"\"\r\n        return self._C\r\n\r\n    @property\r\n    def orthogonal_loadings(self):\r\n        \"\"\" Orthogonal loadings. \"\"\"\r\n        return self._Portho\r\n\r\n    @property\r\n    def orthogonal_scores(self):\r\n        \"\"\" Orthogonal scores. \"\"\"\r\n        return self._Tortho\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/opls.py b/opls.py
--- a/opls.py	
+++ b/opls.py	
@@ -42,6 +42,7 @@
         self._T: Optional[np.ndarray] = None
         self._P: Optional[np.ndarray] = None
         self._C: Optional[np.ndarray] = None
+        self._W: Optional[np.ndarray] = None
         # coefficients
         self.coef: Optional[np.ndarray] = None
         # total number of components
@@ -84,7 +85,9 @@
         Tortho = np.empty((n, npc))
         Portho = np.empty((p, npc))
         Wortho = np.empty((p, npc))
-        T, P, C = np.empty((n, npc)), np.empty((p, npc)), np.empty(npc)
+        T = np.empty((n, npc))
+        P = np.empty((p, npc))
+        C = np.empty(npc)
 
         # X-y variations
         tw = np.dot(y, x) / np.dot(y, y)
Index: cross_validation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nPerform cross validation.\r\n\"\"\"\r\nimport collections\r\nimport typing\r\nimport numpy as np\r\nimport numpy.linalg as la\r\n\r\nimport pretreatment\r\nfrom pls import PLS\r\nfrom opls import OPLS\r\n\r\nimport tqdm\r\n\r\n\r\nclass CrossValidation:\r\n    \"\"\"\r\n    Stratified cross validation\r\n\r\n    Parameters:\r\n    ----------\r\n    estimator: str\r\n        Estimator indicates algorithm for model construction.\r\n        Values can be \"pls\" for PLS and \"opls\" for OPLS. Default\r\n        is \"opls\".\r\n    kfold: int\r\n        k fold cross validation. if k equals to len(X), leave one out\r\n        cross validation will be performed. Default is 10.\r\n    scaler: str\r\n        Scaler for scaling data matrix. Valid values are \"uv\" for\r\n        zero-mean-unit-variance scaling, \"pareto\" for Pareto scaling,\r\n        \"minmax\" for Min-Max scaling and \"mean\" for mean centering.\r\n        Default is \"pareto\".\r\n\r\n    Returns\r\n    -------\r\n    CrossValidation object\r\n\r\n    \"\"\"\r\n    def __init__(self, estimator=\"opls\", kfold=10, scaler=\"pareto\"):\r\n        # number of folds\r\n        self.kfold = kfold\r\n        self._scaler_param: str = scaler\r\n        self._estimator_param: str = estimator\r\n        # estimator\r\n        f_estimator, f_scaler = self._create_scaler_estimator()\r\n        self.estimator = f_estimator\r\n        self.scaler = f_scaler\r\n        self.estimator_id = estimator\r\n        # initialize other attributes, but should be HIDDEN\r\n        self._ypred: typing.Optional[np.ndarray] = None\r\n        self._Tortho: typing.Optional[np.ndarray] = None\r\n        self._Tpred: typing.Optional[np.ndarray] = None\r\n        self._ssx: typing.Optional[dict] = None\r\n        self._ssy: typing.Optional[list] = None\r\n        self._pressy: typing.Optional[np.ndarray] = None\r\n        self._n: typing.Optional[int] = None\r\n        self._pcv: typing.Optional[dict] = None\r\n        self._opt_component: typing.Optional[int] = None\r\n        self._mis_classifications: typing.Optional[np.ndarray] = None\r\n        self._q2: typing.Optional[np.ndarray] = None\r\n        self._npc0: typing.Optional[int] = None\r\n        self._x: typing.Optional[np.ndarray] = None\r\n        self.y: typing.Optional[np.ndarray] = None\r\n        self.groups: typing.Optional[dict] = None\r\n        self._used_variable_index: typing.Optional[np.ndarray] = None\r\n        self._r2x_cum: typing.Optional[float] = None\r\n        self._r2y_cum: typing.Optional[float] = None\r\n        self._corr_y_perms: typing.Optional[np.ndarray] = None\r\n        self._perm_q2: typing.Optional[np.ndarray] = None\r\n        self._perm_err: typing.Optional[np.ndarray] = None\r\n\r\n    def fit(self, x, y):\r\n        \"\"\"\r\n        Fitting variable matrix X\r\n\r\n        Parameters\r\n        ----------\r\n        x : np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        y : np.ndarray | list\r\n            Dependent matrix with size n samples by 1. The values in\r\n            this vector must be 0 and 1, otherwise the classification\r\n            performance will be wrongly concluded.\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        # TODO: Check dimension consistencies between X and y.\r\n        # set the labels in y to 0 and 1, and name the groups using\r\n        # the labels in y\r\n        y = self._reset_y(y)\r\n        # matrix dimension\r\n        n, p = x.shape\r\n        # max number of principal components\r\n        npc0 = min(n, p)\r\n        # preallocation\r\n        ssx = collections.defaultdict(lambda: collections.defaultdict(list))\r\n        ssy = []\r\n        ypred, pressy = np.zeros((n, npc0)), np.zeros((n, npc0))\r\n        tortho, tpred = np.zeros((n, npc0)), np.zeros((n, npc0))\r\n        pcv = collections.defaultdict(list)\r\n        for train_index, test_index in self._split(y):\r\n            xtr, xte = x[train_index], x[test_index]\r\n            ytr, yte = y[train_index], y[test_index]\r\n\r\n            # check the data matrix to remove variables having only 1 unique\r\n            # value.\r\n            val_ix, xtr = self._check_x(xtr)\r\n            xte = xte[:, val_ix]\r\n\r\n            # scale matrix\r\n            xtr_scale = self.scaler.fit(xtr)\r\n            xte_scale = self.scaler.scale(xte)\r\n            ytr_scale = self.scaler.fit(ytr)\r\n            yte_scale = self.scaler.scale(yte)\r\n\r\n            # variances\r\n            ssy_tot = (yte_scale ** 2).sum()\r\n            ssx_tot = (xte_scale ** 2).sum()\r\n\r\n            # fit the model\r\n            npc = min(xtr.shape)\r\n            self.estimator.fit(xtr_scale.copy(), ytr_scale, n_comp=npc)\r\n            if npc < npc0:\r\n                npc0 = npc\r\n\r\n            # do prediction iterating through components\r\n            for k in range(1, npc+1):\r\n                # if OPLS is used, the test matrix should be corrected to\r\n                # remove orthogonal components\r\n                if self._estimator_param == \"opls\":\r\n                    xte_corr, tcorr = self.estimator.correct(\r\n                        xte_scale, n_component=k, return_scores=True\r\n                    )\r\n                    # prediction\r\n                    yp_k, tp_k = self.estimator.predict(\r\n                        xte_corr, n_component=k, return_scores=True\r\n                    )\r\n\r\n                    # save the parameters for model quality assessments\r\n                    # Orthogonal and predictive scores\r\n                    if xte_scale.ndim == 1:\r\n                        tortho[test_index, k-1] = tcorr[0]\r\n                    else:\r\n                        tortho[test_index, k-1] = tcorr[:, 0]\r\n                    tpred[test_index, k-1] = tp_k\r\n\r\n                    # sum of squares\r\n                    ssx[k][\"corr\"].append((xte_corr ** 2).sum())\r\n                    xte_ortho = np.dot(\r\n                        tcorr, self.estimator.orthogonal_loadings[:, :k].T\r\n                    )\r\n                    ssx[k][\"xyo\"].append((xte_ortho ** 2).sum())\r\n                    ssx[k][\"total\"].append(ssx_tot)\r\n\r\n                    # covariances from fitting\r\n                    tp = self.estimator.predictive_scores[:, k-1]\r\n                    pcv[k].append(np.dot(tp, xtr_scale) / (tp ** 2).sum())\r\n\r\n                else:\r\n                    # prediction\r\n                    yp_k = self.estimator.predict(xte_scale, n_component=k)\r\n\r\n                # predicted y\r\n                ypred[test_index, k-1] = yp_k\r\n                pressy[test_index, k-1] = (yp_k - yte_scale) ** 2\r\n\r\n            ssy.append(ssy_tot)\r\n\r\n        # save metrics\r\n        self._ypred = ypred[:, :npc0]\r\n        self._pressy = pressy[:, :npc0]\r\n        self._ssy = sum(ssy)\r\n        self._n = n\r\n        self._npc0 = npc0\r\n        self._x = x\r\n        self.y = y\r\n\r\n        # opls specific metrics\r\n        if self._estimator_param == \"opls\":\r\n            self._Tortho = tortho[:, :npc0]\r\n            self._Tpred = tpred[:, :npc0]\r\n            self._ssx = ssx\r\n            self._pcv = pcv\r\n\r\n        # summarize cross validation results\r\n        self._summary_cv()\r\n        # refit for a final model\r\n        self._create_optimal_model(x, y)\r\n\r\n    def predict(self, x, return_scores=False):\r\n        \"\"\"\r\n        Does prediction using optimal model.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        return_scores: bool\r\n            For OPLS, it's possible to return predictive scores. Thus\r\n            setting this True with `estimator` being \"opls\" will return\r\n            the predictive scores\r\n\r\n        Returns\r\n        -------\r\n        y: np.ndarray\r\n            Predictions for the x\r\n        scores: np.ndarray\r\n            Predictive scores for OPLS\r\n\r\n        \"\"\"\r\n        # TODO: check the dimension consistencies between the training\r\n        #       data and the input data matrix.\r\n        npc = self._opt_component + 1\r\n        # scale the matrix\r\n        x = self.scaler.scale(x[:, self._used_variable_index])\r\n        if self._estimator_param == \"opls\":\r\n            x = self.estimator.correct(x.copy(), n_component=npc)\r\n            return self.estimator.predict(\r\n                x, n_component=npc, return_scores=return_scores\r\n            )\r\n        return self.estimator.predict(x, n_component=npc)\r\n\r\n    def permutation_test(self, num_perms=10000) -> None:\r\n        \"\"\"\r\n        Performs permutation test on constructed model.\r\n\r\n        Parameters\r\n        ----------\r\n        num_perms: int\r\n            Number of permutations. Defaults to 10000.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n\r\n        \"\"\"\r\n        # check the arguments\r\n        if not isinstance(num_perms, int):\r\n            raise ValueError(\"Expected integer, got {}.\".format(num_perms))\r\n        if num_perms < 20:\r\n            raise ValueError(\"Expected large positive integer >= 20, \"\r\n                             \"got {}.\".format(num_perms))\r\n\r\n        is_opls = self._estimator_param == \"opls\"\r\n\r\n        estimator, scaler = self._create_scaler_estimator()\r\n\r\n        # do permutation test\r\n        x = self._x[:, self._used_variable_index]\r\n        # center y\r\n        y_center = self.y - self.y.mean()\r\n        ssy_c = (y_center ** 2).sum()\r\n        # optimal component number\r\n        npc: int = self._opt_component + 1\r\n        n: int = self.y.size\r\n        pred_y: np.ndarray = np.zeros(n, dtype=np.float64)\r\n\r\n        rnd_generator = np.random.default_rng()\r\n\r\n        perm_q2: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        perm_err: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        perm_corr: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        for i in tqdm.tqdm(range(num_perms), total=num_perms,\r\n                           desc=\"Calculating permuted metrics\"):\r\n            # randomize labels\r\n            ix = rnd_generator.permutation(n)\r\n            rnd_y = self.y[ix]\r\n            ssy: float = 0.\r\n            ssey: float = 0.\r\n            # fit the model using cross validation\r\n            for train_index, test_index in self._split(rnd_y):\r\n                xtr, xte = x[train_index], x[test_index]\r\n                ytr, yte = rnd_y[train_index], rnd_y[test_index]\r\n\r\n                # scale matrix\r\n                xtr_scale = scaler.fit(xtr)\r\n                xte_scale = scaler.scale(xte)\r\n                ytr_scale = scaler.fit(ytr)\r\n                yte_scale = scaler.scale(yte)\r\n\r\n                # variances\r\n                ssy += (yte_scale ** 2).sum()\r\n                # fitting the model\r\n                estimator.fit(xtr_scale.copy(), ytr_scale, n_comp=npc)\r\n                # prediction\r\n                if is_opls:\r\n                    xc = estimator.correct(xte_scale.copy(), n_component=npc)\r\n                    yp = estimator.predict(xc, n_component=npc)\r\n                else:\r\n                    yp = estimator.predict(xte_scale)\r\n                pred_y[test_index] = yp\r\n                ssey += ((yp - yte_scale) ** 2).sum()\r\n\r\n            pred_cls = (pred_y > 0.).astype(int)\r\n            perm_err[i] = np.count_nonzero((pred_cls - rnd_y) != 0) / n\r\n            perm_q2[i] = 1. - ssey / ssy\r\n            perm_corr[i] = abs(((y_center * y_center[ix]).sum()) / ssy_c)\r\n\r\n        self._perm_q2 = perm_q2\r\n        self._perm_err = perm_err\r\n        self._corr_y_perms = perm_corr\r\n\r\n    def reset_optimal_num_component(self, k) -> None:\r\n        \"\"\"\r\n        Resets the optimal number of components for manual setup.\r\n\r\n        Parameters\r\n        ----------\r\n        k: int\r\n            Number of components according to the error plot.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n\r\n        \"\"\"\r\n        if not isinstance(k, int) or k <= 0:\r\n            raise ValueError(\"The number must be a positive integer.\")\r\n\r\n        if k > self._npc0:\r\n            raise ValueError(\"The number must not exceed the maximum \"\r\n                             f\" number of components {self._npc0}.\")\r\n\r\n        self._opt_component = k\r\n        # re-fit the model using the updated optimal number of components\r\n        self._create_optimal_model(self._x, self.y)\r\n\r\n    @property\r\n    def orthogonal_score(self) -> np.ndarray:\r\n        \"\"\"\r\n        Returns cross validated orthogonal score.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first orthogonal scores.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._Tortho[:, self._opt_component]\r\n\r\n    @property\r\n    def predictive_score(self) -> np.ndarray:\r\n        \"\"\"\r\n        Returns cross validated predictive score.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive scores.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._Tpred[:, self._opt_component]\r\n\r\n    @property\r\n    def scores(self) -> np.ndarray:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive score, if the method is OPLS/OPLS-DA,\r\n            otherwise is the scores of X\r\n\r\n        \"\"\"\r\n        if self._estimator_param == \"opls\":\r\n            return self.predictive_score\r\n        else:\r\n            return self.estimator.scores_x\r\n\r\n    @property\r\n    def q2(self) -> float:\r\n        \"\"\"\r\n        Returns cross validated Q2.\r\n\r\n        Returns\r\n        -------\r\n        q2: float\r\n\r\n        \"\"\"\r\n        return float(self._q2[self._opt_component])\r\n\r\n    @property\r\n    def optimal_component_num(self) -> int:\r\n        \"\"\"\r\n        Number of components determined by cross validation.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n\r\n        \"\"\"\r\n        return self._opt_component + 1\r\n\r\n    @property\r\n    def R2Xcorr(self) -> float:\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled joint X-y covariation of X.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._r2xcorr[self._opt_component]\r\n\r\n    @property\r\n    def R2XYO(self) -> float:\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled structured noise variation of X.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._r2xyo[self._opt_component]\r\n\r\n    @property\r\n    def R2X(self) -> float:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled variation of X\r\n\r\n        \"\"\"\r\n        return self._r2x\r\n\r\n    @property\r\n    def R2y(self) -> float:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled variation of y\r\n\r\n        \"\"\"\r\n        return self._r2y\r\n\r\n    @property\r\n    def R2X_cum(self) -> float:\r\n        \"\"\"\r\n        Cumulative fraction of the sum of squares explained up to the\r\n        optimal number of principal components.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Cumulative fraction of the sum of squares explained\r\n\r\n        \"\"\"\r\n        return self._r2x_cum\r\n\r\n    @property\r\n    def R2y_cum(self) -> float:\r\n        \"\"\"\r\n        Cumulative fraction of the sum of squares explained up to the\r\n        optimal number of principal components.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Cumulative fraction of the sum of squares explained\r\n\r\n        \"\"\"\r\n        return self._r2y_cum\r\n\r\n    @property\r\n    def correlation(self) -> np.ndarray:\r\n        \"\"\" Correlation\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        References\r\n        ----------\r\n        [1] Wiklund S, et al. Visualization of GC/TOF-MS-Based\r\n        Metabolomics Data for Identification of Biochemically\r\n        Interesting Compounds Using OPLS Class Models. Anal Chem.\r\n        2008, 80, 115-122.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._corr\r\n\r\n    @property\r\n    def covariance(self):\r\n        \"\"\"\r\n        Covariance\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        References\r\n        ----------\r\n        [1] Wiklund S, et al. Visualization of GC/TOF-MS-Based\r\n        Metabolomics Data for Identification of Biochemically\r\n        Interesting Compounds Using OPLS Class Models. Anal Chem.\r\n        2008, 80, 115-122.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._cov\r\n\r\n    @property\r\n    def loadings_cv(self):\r\n        \"\"\"\r\n        Loadings from cross validation.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return np.array(self._pcv[self._opt_component+1])\r\n\r\n    @property\r\n    def min_nmc(self) -> int:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Minimal number of mis-classifications obtained by\r\n            cross validation.\r\n\r\n        \"\"\"\r\n        return int(self._mis_classifications[self._opt_component])\r\n\r\n    @property\r\n    def mis_classifications(self) -> np.ndarray:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        list\r\n            Mis-classifications at different principal components.\r\n\r\n        \"\"\"\r\n        return self._mis_classifications\r\n\r\n    @property\r\n    def used_variable_index(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Indices of variables used for model construction\r\n\r\n        \"\"\"\r\n        return self._used_variable_index\r\n\r\n    @property\r\n    def permutation_q2(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Q2 array generated by permutation test.\r\n\r\n        \"\"\"\r\n        if self._perm_q2 is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._perm_q2\r\n\r\n    @property\r\n    def permutation_error(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Misclassification error rates generated by permutation test.\r\n\r\n        \"\"\"\r\n        if self._perm_err is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._perm_err\r\n\r\n    @property\r\n    def correlation_permute_y(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Correlation between permuted y and normal y.\r\n\r\n        \"\"\"\r\n        if self._corr_y_perms is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._corr_y_perms\r\n\r\n    def p(self, metric=\"q2\"):\r\n        \"\"\"\r\n        Calculates the significance of the constructed model by\r\n        permutation test.\r\n\r\n        Parameters\r\n        ----------\r\n        metric: str\r\n            Metric used to assess the performance of the constructed\r\n            model. \"q2\" and \"error\" are accepted as values.\r\n            \"q2\": Q2\r\n            \"error\": Misclassification error rate.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            p value\r\n\r\n        \"\"\"\r\n        if self._perm_err is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        if metric not in (\"q2\", \"error\"):\r\n            raise ValueError(\"Expected `q2`, `error`, got {}.\".format(metric))\r\n\r\n        if metric == \"q2\":\r\n            nb: int = np.count_nonzero(self._perm_q2 >= self.q2) + 1\r\n            nt: float = self._perm_q2.size + 1.\r\n        else:\r\n            err: float = self.min_nmc / self.y.size\r\n            nb: int = np.count_nonzero(self._perm_err <= err) + 1\r\n            nt: float = self._perm_err.size + 1.\r\n\r\n        return nb / nt\r\n\r\n    @staticmethod\r\n    def _check_x(x) -> typing.Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"\r\n        Checks the valid variables to remove those with unique value.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Data matrix\r\n\r\n        Returns\r\n        -------\r\n        index: np.ndarray\r\n            Indices of valid variables\r\n        x: np.ndarray\r\n            Valid data matrix\r\n\r\n        \"\"\"\r\n        # check nan and inf\r\n        has_nan = np.isnan(x).any(axis=0)\r\n        has_inf = np.isinf(x).any(axis=0)\r\n        idx, = np.where(~(has_nan | has_inf))\r\n        # check unique value\r\n        is_unique_value = np.absolute(\r\n            x[:, idx] - x[:, idx].mean(axis=0)\r\n        ).sum(axis=0) == 0\r\n        # index of valid variables\r\n        idx = idx[~is_unique_value]\r\n\r\n        return idx, x[:, idx]\r\n\r\n    def _split(self, y) -> typing.Iterable:\r\n        \"\"\"\r\n        Split total number of n samples into training and testing data.\r\n\r\n        Parameters\r\n        ----------\r\n        y: np.ndarray\r\n            Number of samples\r\n\r\n        Returns\r\n        -------\r\n        iterator\r\n\r\n        \"\"\"\r\n        n, k = y.size, self.kfold\r\n        groups, counts = np.unique(y, return_counts=True)\r\n\r\n        # check the number\r\n        if counts.min() < k and n != k:\r\n            raise ValueError(f\"The fold number {k} is larger than the least\"\r\n                             f\" group number {counts.min()}.\")\r\n\r\n        indices = np.arange(n, dtype=int)\r\n        # leave one out cross validation\r\n        if n == k:\r\n            for i in indices:\r\n                yield np.delete(indices, i), [i]\r\n\r\n        # k fold cross validation\r\n        else:\r\n            group_index, blks = [], []\r\n            for g, nk in zip(groups, counts):\r\n                group_index.append(np.where(y == g)[0])\r\n                blks.append(nk // k if nk % k == 0 else nk // k + 1)\r\n            # splitting data\r\n            for i in range(k):\r\n                trains = np.ones(n, dtype=bool)\r\n                for blk, idx, nk in zip(blks, group_index, counts):\r\n                    trains[idx[blk * i: min(blk * (i + 1), nk)]] = False\r\n                yield indices[trains], indices[np.logical_not(trains)]\r\n\r\n    def _create_optimal_model(self, x, y) -> None:\r\n        \"\"\"\r\n        Create final model based on the optimal number of components.\r\n        \"\"\"\r\n        val_ix, x = self._check_x(x)\r\n\r\n        # scale data matrix\r\n        y_scale = self.scaler.fit(y)\r\n        x_scale = self.scaler.fit(x)\r\n\r\n        # optimal component number\r\n        npc = self._opt_component+1\r\n\r\n        # fit the model\r\n        self.estimator.fit(x_scale.copy(), y_scale.copy(), n_comp=npc)\r\n\r\n        # summary the fitting\r\n        self._summary_fit(x_scale, y_scale)\r\n\r\n        # indices of variables used for model construction\r\n        self._used_variable_index = val_ix\r\n\r\n    def _summary_fit(self, x, y) -> None:\r\n        \"\"\"\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            scaled variable matrix.\r\n        y: np.ndarray\r\n            scaled dependent variable\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        npc = self._opt_component + 1\r\n        # Calculate covariance and correlation for variable importance\r\n        # assessment. Only works for OPLS/OPLS-DA\r\n        r2x_pc: np.ndarray = np.zeros(npc, dtype=np.float64)\r\n        r2y_pc: np.ndarray = np.zeros(npc, dtype=np.float64)\r\n        ssx: float = (x ** 2).sum()\r\n        ssy: float = (y ** 2).sum()\r\n        if self._estimator_param == \"opls\":\r\n            tp = self.estimator.predictive_score(npc)\r\n            ss_tp = np.dot(tp, tp)\r\n            # loadings\r\n            w = np.dot(tp, x)\r\n            self._cov = w / ss_tp\r\n            self._corr = w / (np.sqrt(ss_tp) * la.norm(x, axis=0))\r\n\r\n            # reconstruct variable matrix X\r\n            # from orthogonal corrections.\r\n            o_scores = self.estimator.orthogonal_scores\r\n            o_loads = self.estimator.orthogonal_loadings\r\n            p_scores = self.estimator.predictive_scores\r\n            p_loads = self.estimator.predictive_loadings\r\n            for i in range(npc):\r\n                xrec = np.dot(o_scores[:, i][:, np.newaxis],\r\n                              o_loads[:, i][np.newaxis, :])\r\n                # from predictive scores\r\n                xrec += np.dot(p_scores[:, i][:, np.newaxis],\r\n                               p_loads[:, i][np.newaxis, :])\r\n                r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx\r\n\r\n                # reconstruct dependent vector y\r\n                yrec = p_scores[:, i] * self.estimator.weights_y[i]\r\n                r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy\r\n        else:\r\n            for i in range(npc):\r\n                xrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],\r\n                              self.estimator.loadings_x[:, i][:, np.newaxis])\r\n                yrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],\r\n                              self.estimator.weights_y[i])\r\n                r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx\r\n                r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy\r\n\r\n        # r2x\r\n        self._r2x = 1. - r2x_pc[self._opt_component]\r\n        # r2y\r\n        self._r2y = 1. - r2y_pc[self._opt_component]\r\n        # cumulative r2x\r\n        self._r2x_cum = 1. - np.prod(r2x_pc)\r\n        # cumulative r2y\r\n        self._r2y_cum = 1. - np.prod(r2y_pc)\r\n\r\n    def _summary_cv(self) -> None:\r\n        \"\"\"\r\n        Summary cross validation results to calculate metrics for\r\n        assessing the model.\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        # number of mis-classifications\r\n        _pred_class = (self._ypred > 0).astype(float)\r\n        nmc = ((_pred_class - self.y[:, np.newaxis]) != 0).sum(axis=0)\r\n        j = int(np.argmin(nmc))\r\n        # optimal number of components\r\n        self._opt_component: int = j\r\n        self._mis_classifications = nmc\r\n        # Q2\r\n        self._q2 = 1. - self._pressy.sum(axis=0) / self._ssy\r\n        # metrics for OPLS\r\n        if self._estimator_param == \"opls\":\r\n            _, npc = _pred_class.shape\r\n            # r2xcorr, r2xyo\r\n            r2xcorr, r2xyo = [], []\r\n            for k in range(1, npc+1):\r\n                r2xcorr.append(\r\n                    sum(self._ssx[k][\"corr\"]) / sum(self._ssx[k][\"total\"])\r\n                )\r\n                r2xyo.append(\r\n                    sum(self._ssx[k][\"xyo\"]) / sum(self._ssx[k][\"total\"])\r\n                )\r\n            self._r2xcorr = r2xcorr\r\n            self._r2xyo = r2xyo\r\n\r\n    def _reset_y(self, y) -> np.ndarray:\r\n        \"\"\"\r\n        Reset the labels in y to 0 and 1, and name the groups using the\r\n        labels in y.\r\n\r\n        Parameters\r\n        ----------\r\n        y: np.ndarray | list\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Label reset in y.\r\n\r\n        \"\"\"\r\n        if isinstance(y, list):\r\n            y = np.array([str(v) for v in y], dtype=str)\r\n\r\n        # groups\r\n        labels = np.unique(y)\r\n        # only binary classification is allowed.\r\n        if labels.size != 2:\r\n            raise ValueError(\r\n                \"Only binary classification is currently accepted.\"\r\n            )\r\n\r\n        # reset the values for each class\r\n        groups = collections.defaultdict()\r\n        y_reset = np.zeros_like(y, dtype=float)\r\n        for i, label in enumerate(labels):\r\n            y_reset[y == label] = i\r\n            groups[i] = label if isinstance(label, str) else str(int(label))\r\n\r\n        self.groups = groups\r\n        return y_reset\r\n\r\n    def _create_scaler_estimator(self):\r\n        \"\"\"\r\n        Creates scaler and estimator.\r\n\r\n        Returns\r\n        -------\r\n\r\n        \"\"\"\r\n        if self._estimator_param == \"pls\":\r\n            return PLS(), pretreatment.Scaler(scaler=self._scaler_param)\r\n        if self._estimator_param == \"opls\":\r\n            return OPLS(), pretreatment.Scaler(scaler=self._scaler_param)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cross_validation.py b/cross_validation.py
--- a/cross_validation.py	
+++ b/cross_validation.py	
@@ -69,6 +69,7 @@
         self._corr_y_perms: typing.Optional[np.ndarray] = None
         self._perm_q2: typing.Optional[np.ndarray] = None
         self._perm_err: typing.Optional[np.ndarray] = None
+        self._vip: typing.Optional[np.ndarray] = None
 
     def fit(self, x, y):
         """
@@ -754,6 +755,52 @@
                     trains[idx[blk * i: min(blk * (i + 1), nk)]] = False
                 yield indices[trains], indices[np.logical_not(trains)]
 
+    def _cal_vip(self) -> None:
+        """
+        Calculates variable importance in projection (VIP).
+        """
+        npc = self._opt_component + 1
+        p = self._x.shape[1]
+        w_weights: np.ndarray = np.zeros((npc, p), dtype=np.float64)
+        if self._scaler_param == "uv":
+            # already standardized to zero mean and unit variance,
+            # directly use the results
+            if self._estimator_param == "opls":
+                tp = self.estimator.predictive_score(npc)
+                ss_tp = np.dot(tp, tp)
+                # loadings
+                w = np.dot(tp, x)
+                self._cov = w / ss_tp
+                self._corr = w / (np.sqrt(ss_tp) * la.norm(x, axis=0))
+
+                # reconstruct variable matrix X
+                # from orthogonal corrections.
+                o_scores = self.estimator.orthogonal_scores
+                o_loads = self.estimator.orthogonal_loadings
+                p_scores = self.estimator.predictive_scores
+                p_loads = self.estimator.predictive_loadings
+                for i in range(npc):
+                    xrec = np.dot(o_scores[:, i][:, np.newaxis],
+                                  o_loads[:, i][np.newaxis, :])
+                    # from predictive scores
+                    xrec += np.dot(p_scores[:, i][:, np.newaxis],
+                                   p_loads[:, i][np.newaxis, :])
+                    r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx
+
+                    # reconstruct dependent vector y
+                    yrec = p_scores[:, i] * self.estimator.weights_y[i]
+                    r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy
+            else:
+                ssy_exp: float = 0.
+                for i in range(npc):
+                    yrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],
+                                  self.estimator.weights_y[i])
+                    ssk = (yrec ** 2).sum()
+                    ssy_exp += ssk
+                    w_weights[i] = (self.estimator.weights_x[:, i] ** 2) * ssk
+                vips = np.sqrt(w_weights.sum(axis=0) * p / ssy_exp)
+        self._vip = vips.copy()
+
     def _create_optimal_model(self, x, y) -> None:
         """
         Create final model based on the optimal number of components.
Index: .idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]/shelved.patch	
+++ /dev/null	
@@ -1,133 +0,0 @@
-Index: .idea/inspectionProfiles/Project_Default.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/inspectionProfiles/Project_Default.xml b/.idea/inspectionProfiles/Project_Default.xml
-new file mode 100644
---- /dev/null	(date 1704957769200)
-+++ b/.idea/inspectionProfiles/Project_Default.xml	(date 1704957769200)
-@@ -0,0 +1,10 @@
-+<component name="InspectionProjectProfileManager">
-+  <profile version="1.0">
-+    <option name="myName" value="Project Default" />
-+    <inspection_tool class="DuplicatedCode" enabled="true" level="WEAK WARNING" enabled_by_default="true">
-+      <Languages>
-+        <language minSize="98" name="Python" />
-+      </Languages>
-+    </inspection_tool>
-+  </profile>
-+</component>
-\ No newline at end of file
-Index: .idea/pypls.iml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/pypls.iml b/.idea/pypls.iml
-new file mode 100644
---- /dev/null	(date 1704957769220)
-+++ b/.idea/pypls.iml	(date 1704957769220)
-@@ -0,0 +1,12 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<module type="PYTHON_MODULE" version="4">
-+  <component name="NewModuleRootManager">
-+    <content url="file://$MODULE_DIR$" />
-+    <orderEntry type="jdk" jdkName="Python 3.10" jdkType="Python SDK" />
-+    <orderEntry type="sourceFolder" forTests="false" />
-+  </component>
-+  <component name="PyDocumentationSettings">
-+    <option name="format" value="NUMPY" />
-+    <option name="myDocStringFormat" value="NumPy" />
-+  </component>
-+</module>
-\ No newline at end of file
-Index: .idea/modules.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/modules.xml b/.idea/modules.xml
-new file mode 100644
---- /dev/null	(date 1704957769232)
-+++ b/.idea/modules.xml	(date 1704957769232)
-@@ -0,0 +1,8 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="ProjectModuleManager">
-+    <modules>
-+      <module fileurl="file://$PROJECT_DIR$/.idea/pypls.iml" filepath="$PROJECT_DIR$/.idea/pypls.iml" />
-+    </modules>
-+  </component>
-+</project>
-\ No newline at end of file
-Index: .idea/misc.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/misc.xml b/.idea/misc.xml
-new file mode 100644
---- /dev/null	(date 1704957769253)
-+++ b/.idea/misc.xml	(date 1704957769253)
-@@ -0,0 +1,7 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="Black">
-+    <option name="sdkName" value="Python 3.10" />
-+  </component>
-+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.10" project-jdk-type="Python SDK" />
-+</project>
-\ No newline at end of file
-Index: .idea/inspectionProfiles/profiles_settings.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
-new file mode 100644
---- /dev/null	(date 1704957769265)
-+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1704957769265)
-@@ -0,0 +1,6 @@
-+<component name="InspectionProjectProfileManager">
-+  <settings>
-+    <option name="USE_PROJECT_PROFILE" value="false" />
-+    <version value="1.0" />
-+  </settings>
-+</component>
-\ No newline at end of file
-Index: .idea/vcs.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/vcs.xml b/.idea/vcs.xml
-new file mode 100644
---- /dev/null	(date 1704957769277)
-+++ b/.idea/vcs.xml	(date 1704957769277)
-@@ -0,0 +1,6 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="VcsDirectoryMappings">
-+    <mapping directory="" vcs="Git" />
-+  </component>
-+</project>
-\ No newline at end of file
-Index: .idea/.gitignore
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/.gitignore b/.idea/.gitignore
-new file mode 100644
---- /dev/null	(date 1704957769297)
-+++ b/.idea/.gitignore	(date 1704957769297)
-@@ -0,0 +1,8 @@
-+# Default ignored files
-+/shelf/
-+/workspace.xml
-+# Editor-based HTTP Client requests
-+/httpRequests/
-+# Datasource local storage ignored files
-+/dataSources/
-+/dataSources.local.xml
Index: .idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]/shelved.patch	
+++ /dev/null	
@@ -1,157 +0,0 @@
-Index: readme.md
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+># pypls\r\nThis package implements PLS-DA and OPLS-DA for analysis of\r\nhigh-dimensional data derived from, for example, mass spectrometry\r\nin metabolomics. The visualization of score plots, S-plot, jack-knife\r\nconfidence intervals for loading profile, and mis-classification number\r\nin cross validation are also implemented.\r\n## Prerequisites\r\nThis package is created by ```Python 3.7```, with the following packages\r\nrequired:\r\n```\r\nnumpy 1.17.2\r\nscipy 1.3.1\r\nmatplotlib 3.1.3\r\ntqdm 4.64.0\r\n```\r\nAll of these or newer version packages can be installed by using ``pip``.\r\n## Important\r\nThis package is only workable for binary classifications. Thus, if three or\r\nmore classes are in the data, this package can't handle that. An alternative\r\nway is pair-wise classifications. As *Prof. Richard G. Brereton*\r\npointed out in his paper<sup>[1]</sup>, binary classification is recommended for PLS\r\nrelated methods, and multi-class classification problems are not suitable\r\nfor PLS. \r\n## Install\r\nThe latest release can be downloaded\r\n[**here**](https://github.com/DongElkan/pypls/releases).\r\nUncompress the package and set `Python` working directory there.\r\nSince current version is not packaged, all modules must be run\r\nunder the working directory.\r\n## Running the codes\r\n```\r\n# import cross validation module\r\nimport cross_validation\r\n# import plotting functions\r\nimport plotting\r\n``` \r\n1. Initialize cross validation object for 10-fold cross validation using\r\nOPLS-DA.\r\n    ```\r\n    cv = cross_validation.CrossValidation(kfold=10, estimator=\"opls\")\r\n    ```\r\n    Parameters:  \r\n    `kfold`: Fold in cross validation. For leave-one-out cross validation,\r\n    set it to `n`, is the number of samples.  \r\n    `estimator`: The classifier, valid values are `opls` and `pls`. Defaults to `opls`.  \r\n    `scaler`: scaling of variable matrix.    \r\n     * `uv`: zero mean and unit variance scaling.  \r\n     * `pareto`: Pareto scaling. *This is the default.*  \r\n     * `minmax`: min-max scaling so that the range for each variable is\r\n     between 0 and 1.  \r\n     * `mean`: zero mean scaling.\r\n2. Fit the model.\r\n   ```\r\n   cv.fit(X, labels)\r\n   ```\r\n   `X` is the variable matrix with size `n` (rows) by `p` (columns), where\r\n   `n` is number of samples and `p` is number of variables.\r\n   `labels` can be numeric values or strings, with number of\r\n   elements equals to `n`.\r\n3. Permutation test <sup>[5, 6]</sup>    \r\n    To identify whether the constructed model is overfitting, permutation\r\ntest is generally applied, by repeatedly simply randomizing labels and performing\r\nthe model construction and prediction on the randomized labels many times. This\r\npackage adopts same strategy, which uses\r\n    ```\r\n    cv.permutation_test()\r\n    ```\r\n    Parameters:  \r\n    `num_perms`: Number of permutations. Defaults to `10000`.  \r\n    `metric`: Metric used to assess the performance of the constructed model. Valid\r\nvalues are `q2` and `error`, where `q2` calculates Q2 and `error` calculates the\r\nmis-classification error. Defaults to `q2`.\r\n4. Visualization of results.\r\n    ```\r\n    # construct the plotting object\r\n    plots = plotting.Plots(cv)\r\n    ```\r\n    * Number of mis-classifications at different principal components:\r\n    ```\r\n    plots.plot_cv_errors()\r\n    ```\r\n    * Cross validated score plot:\r\n    ```\r\n    plots.plot_scores()\r\n    ```\r\n   > [!NOTE]  \r\n   > For OPLS-DA, predictive scores `tp` vs the first orthogonal\r\n    scores `to` will be shown; for PLS, the first and second component will\r\n    be shown.\r\n    * S-plot (only suitable for OPLS-DA).\r\n    ```\r\n    plots.splot()\r\n    ```\r\n    * Loading profile with Jack-knife confidence intervals (only suitable for OPLS-DA).\r\n    ```\r\n    means, intervals = plots.jackknife_loading_plot(alpha=0.05)\r\n    ```\r\n    Where `alpha` is significance level, default is `0.05`.\r\n    `means` are mean loadings, and `intervals` are\r\n    Jack-knife confidence intervals.  \r\n    * Permutation plot\r\n    ```\r\n   plots.plot_permutation_test()\r\n   ```\r\n   Two subplots will be generated to show the permutation test results:  \r\n    - [x] _Correlation of permuted y to original y_ vs _Model metric_.\r\n    - [x] **Distribution of permutation model metric** which is used to calculate _p_ value. \r\n   > [!IMPORTANT]  \r\n   > It should be noted that, the metric value shown in the plot can be different with that obtained\r\nfrom cross validation, _e.g._, Q2. This is because in permutation test, all metrics are obtained from\r\nself-prediction results, _i.e._, models are constructed using the input dateset and cross validation\r\nparameters and predict same set of data. **_Therefore, the metric should be higher than that obtained\r\nfrom cross validation_**, but should be consistent with that practiced during permutation test. _p_\r\nvalue is then calculated as  \r\n$$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$\r\n    \r\n   > [!NOTE]  \r\n   > For all these plots, set `save_plot=True` and `file_name=some_string.png`\r\ncan save each plot to `some_string.png` with `dpi=1200`.\r\n5. Model assessment\r\n    ```\r\n    # R2X\r\n    cv.R2X_cum\r\n    # Q2\r\n    cv.q2\r\n    # R2y\r\n    cv.R2y_cum\r\n    # Number of mis-classifications\r\n    cv.min_nmc\r\n    ```\r\n   To check the `R2X` and `R2y` of the optimal component, _i.e._,\r\n`cv.optimal_component_num`, call `cv.R2X` and `cv.R2y`.\r\n6. Access other metrics\r\n    * Cross validated predictive scores: `cv.scores`\r\n    * Cross validated predictive loadings: `cv.loadings_cv`\r\n    * Optimal number of components determined by cross\r\n    validation: `cv.optimal_component_num`\r\n7. Prediction of new data\r\n    ```\r\n    predicted_scores = cv.predict(X, return_scores=False)\r\n    ```\r\n    To predict the class, use\r\n    ```\r\n    predicted_groups = (predicted_scores >= 0).astype(int)\r\n    ```\r\n    This will output values of `0` and `1` to indicate the\r\n    groups of samples submitted for prediction. `cv` object\r\n    has the attribute `groups` storing the group names which\r\n    were assigned in `labels` input for training. To access the\r\n    group names after prediction, use\r\n    ```\r\n    print([cv.groups[g] for g in predicted_groups])\r\n    ```\r\n    Set `return_scores=True` will return predictive scores for OPLS.\r\n8. Other methods  \r\n    `cv` provides a method `reset_optimal_num_component` to reset\r\n    the optimal number of components manually, instead of defaultedly\r\n    at the minimal number of mis-classification.\r\n    ```\r\n    cv.reset_optimal_num_component(n)\r\n    ```\r\n\r\n## Author\r\nNai-ping Dong\r\nEmail: naiping.dong@hotmail.com\r\n\r\n## License\r\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/DongElkan/pypls/blob/master/LICENSE) for details.\r\n\r\n## References\r\n[1] Brereton RG, Lloyd GR. Partial least squares discriminant analysis:\r\ntaking the magic away. *J Chemometr*. 2014, 18, 213-225.\r\n[Link](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2609)  \r\n[2] Trygg J, Wold S. Projection on Latent Structure (O-PLS). *J\r\nChemometr*. 2002, 16, 119-128.\r\n[Link](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.695)   \r\n[3] Trygg J, Wold S. O2-PLS, a two-block (X-Y) latent variable regression\r\n(LVR) method with a integral OSC filter. *J Chemometr*. 2003, 17, 53-64.\r\n[Link](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.775)  \r\n[4] Wiklund S, *et al*. Visualization of GC/TOF-MS-Based Metabolomics\r\nData for Identification of Biochemically Interesting Compounds Using\r\nOPLS Class Models. *Anal Chem*. 2008, 80, 115-122.\r\n[Link](https://pubs.acs.org/doi/abs/10.1021/ac0713510)\r\n[5] Bijlsma S, *et al*. Large-Scale Human Metabolomics Studies: A Strategy for\r\nData (Pre-) Processing and Validation. *Anal Chem*. 2006, 78, 2, 567574.\r\n[Link](https://pubs.acs.org/doi/10.1021/ac051495j)  \r\n[6] Ojala M, *et al*. Permutation Tests for Studying Classifier Performance.\r\n*J Mach Learn Res*. 2010, 11, 18331863.\r\n[Link](https://www.jmlr.org/papers/v11/ojala10a.html)
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/readme.md b/readme.md
---- a/readme.md	(revision aadfe75f07f7d2732c558938806f6f70c1ec40ed)
-+++ b/readme.md	(date 1704957735238)
-@@ -180,10 +180,10 @@
- [4] Wiklund S, *et al*. Visualization of GC/TOF-MS-Based Metabolomics
- Data for Identification of Biochemically Interesting Compounds Using
- OPLS Class Models. *Anal Chem*. 2008, 80, 115-122.
--[Link](https://pubs.acs.org/doi/abs/10.1021/ac0713510)
-+[Link](https://pubs.acs.org/doi/abs/10.1021/ac0713510)  
- [5] Bijlsma S, *et al*. Large-Scale Human Metabolomics Studies: A Strategy for
- Data (Pre-) Processing and Validation. *Anal Chem*. 2006, 78, 2, 567574.
- [Link](https://pubs.acs.org/doi/10.1021/ac051495j)  
- [6] Ojala M, *et al*. Permutation Tests for Studying Classifier Performance.
- *J Mach Learn Res*. 2010, 11, 18331863.
--[Link](https://www.jmlr.org/papers/v11/ojala10a.html)
-\ No newline at end of file
-+[Link](https://www.jmlr.org/papers/v11/ojala10a.html)
-Index: .idea/inspectionProfiles/Project_Default.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/inspectionProfiles/Project_Default.xml b/.idea/inspectionProfiles/Project_Default.xml
-new file mode 100644
---- /dev/null	(date 1704871486061)
-+++ b/.idea/inspectionProfiles/Project_Default.xml	(date 1704871486061)
-@@ -0,0 +1,10 @@
-+<component name="InspectionProjectProfileManager">
-+  <profile version="1.0">
-+    <option name="myName" value="Project Default" />
-+    <inspection_tool class="DuplicatedCode" enabled="true" level="WEAK WARNING" enabled_by_default="true">
-+      <Languages>
-+        <language minSize="98" name="Python" />
-+      </Languages>
-+    </inspection_tool>
-+  </profile>
-+</component>
-\ No newline at end of file
-Index: .idea/pypls.iml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/pypls.iml b/.idea/pypls.iml
-new file mode 100644
---- /dev/null	(date 1704871486210)
-+++ b/.idea/pypls.iml	(date 1704871486210)
-@@ -0,0 +1,12 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<module type="PYTHON_MODULE" version="4">
-+  <component name="NewModuleRootManager">
-+    <content url="file://$MODULE_DIR$" />
-+    <orderEntry type="jdk" jdkName="Python 3.10" jdkType="Python SDK" />
-+    <orderEntry type="sourceFolder" forTests="false" />
-+  </component>
-+  <component name="PyDocumentationSettings">
-+    <option name="format" value="NUMPY" />
-+    <option name="myDocStringFormat" value="NumPy" />
-+  </component>
-+</module>
-\ No newline at end of file
-Index: .idea/modules.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/modules.xml b/.idea/modules.xml
-new file mode 100644
---- /dev/null	(date 1704871486239)
-+++ b/.idea/modules.xml	(date 1704871486239)
-@@ -0,0 +1,8 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="ProjectModuleManager">
-+    <modules>
-+      <module fileurl="file://$PROJECT_DIR$/.idea/pypls.iml" filepath="$PROJECT_DIR$/.idea/pypls.iml" />
-+    </modules>
-+  </component>
-+</project>
-\ No newline at end of file
-Index: .idea/misc.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/misc.xml b/.idea/misc.xml
-new file mode 100644
---- /dev/null	(date 1704877973865)
-+++ b/.idea/misc.xml	(date 1704877973865)
-@@ -0,0 +1,7 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="Black">
-+    <option name="sdkName" value="Python 3.10" />
-+  </component>
-+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.10" project-jdk-type="Python SDK" />
-+</project>
-\ No newline at end of file
-Index: .idea/inspectionProfiles/profiles_settings.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
-new file mode 100644
---- /dev/null	(date 1704871486314)
-+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1704871486314)
-@@ -0,0 +1,6 @@
-+<component name="InspectionProjectProfileManager">
-+  <settings>
-+    <option name="USE_PROJECT_PROFILE" value="false" />
-+    <version value="1.0" />
-+  </settings>
-+</component>
-\ No newline at end of file
-Index: .idea/vcs.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/vcs.xml b/.idea/vcs.xml
-new file mode 100644
---- /dev/null	(date 1704871486342)
-+++ b/.idea/vcs.xml	(date 1704871486342)
-@@ -0,0 +1,6 @@
-+<?xml version="1.0" encoding="UTF-8"?>
-+<project version="4">
-+  <component name="VcsDirectoryMappings">
-+    <mapping directory="" vcs="Git" />
-+  </component>
-+</project>
-\ No newline at end of file
-Index: .idea/.gitignore
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/.gitignore b/.idea/.gitignore
-new file mode 100644
---- /dev/null	(date 1704871486569)
-+++ b/.idea/.gitignore	(date 1704871486569)
-@@ -0,0 +1,8 @@
-+# Default ignored files
-+/shelf/
-+/workspace.xml
-+# Editor-based HTTP Client requests
-+/httpRequests/
-+# Datasource local storage ignored files
-+/dataSources/
-+/dataSources.local.xml
Index: .idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm__Changes_.xml
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm__Changes_.xml b/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm__Changes_.xml
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm__Changes_.xml	
+++ /dev/null	
@@ -1,4 +0,0 @@
-<changelist name="Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]" date="1704957769493" recycled="true" deleted="true">
-  <option name="PATH" value="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_3_22_pm_[Changes]/shelved.patch" />
-  <option name="DESCRIPTION" value="Uncommitted changes before Update at 11/1/2024 3:22 pm [Changes]" />
-</changelist>
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm__Changes_.xml
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm__Changes_.xml b/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm__Changes_.xml
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm__Changes_.xml	
+++ /dev/null	
@@ -1,4 +0,0 @@
-<changelist name="Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]" date="1704973295385" recycled="true" deleted="true">
-  <option name="PATH" value="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_1_2024_7_41_pm_[Changes]/shelved.patch" />
-  <option name="DESCRIPTION" value="Uncommitted changes before Update at 11/1/2024 7:41 pm [Changes]" />
-</changelist>
\ No newline at end of file
