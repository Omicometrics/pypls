Index: pls.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport numpy.linalg as la\r\n\r\nfrom typing import Optional\r\n\r\nfrom base import nipals\r\n\r\n\r\nclass PLS:\r\n    \"\"\" Partial least squares. \"\"\"\r\n    def __init__(self):\r\n        self._T: Optional[np.ndarry] = None\r\n        self._P: Optional[np.ndarry] = None\r\n        self._W: Optional[np.ndarry] = None\r\n        self._C: Optional[np.ndarry] = None\r\n        self.coef: Optional[np.ndarry] = None\r\n\r\n    def fit(self, x, y, n_comp=None) -> None:\r\n        \"\"\"\r\n        Fit PLS model\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n by p, where n number\r\n            of samples/instances, p number of variables\r\n        y: np.ndarray\r\n            Dependent variable with size n by 1\r\n        n_comp: int\r\n            Number of components. Default is None, which indicates that\r\n            smaller number between n and p will be used.\r\n\r\n        Returns\r\n        -------\r\n        PLS object\r\n\r\n        \"\"\"\r\n        n, r = x.shape\r\n        # pre-allocation\r\n        T = np.empty((n, n_comp))\r\n        P = np.empty((r, n_comp))\r\n        W = np.empty((r, n_comp))\r\n        C = np.empty(n_comp)\r\n        # iterate through components\r\n        for nc in range(n_comp):\r\n            w, u, c, t = nipals(x, y)\r\n            # loadings\r\n            p = np.dot(t, x) / np.dot(t, t)\r\n            # update data matrix for next component\r\n            x -= t[:, np.newaxis] * p\r\n            y -= t * c\r\n            # save to matrix\r\n            T[:, nc] = t\r\n            P[:, nc] = p\r\n            W[:, nc] = w\r\n            C[nc] = c\r\n\r\n        # save results to matrix\r\n        self._T = T\r\n        self._P = P\r\n        self._W = W\r\n        self._C = C\r\n\r\n        # coefficients\r\n        # noinspection SpellCheckingInspection\r\n        coefs = np.empty((n_comp, r))\r\n        for nc in range(n_comp):\r\n            coefs[nc] = np.dot(np.dot(\r\n                W[:, :nc], la.inv(np.dot(P[:, :nc].T, W[:, :nc]))\r\n            ), C[:nc])\r\n        self.coef = coefs\r\n\r\n    def predict(self, x, n_component=None) -> np.ndarray:\r\n        \"\"\" Do prediction. \"\"\"\r\n        npc = self.coef.shape[1] - 1\r\n        if n_component is not None and n_component < npc:\r\n            npc = n_component - 1\r\n        coef = self.coef[npc]\r\n        return np.dot(x, coef)\r\n\r\n    @property\r\n    def scores_x(self):\r\n        \"\"\" Scores.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Scores\r\n\r\n        \"\"\"\r\n        return self._T\r\n\r\n    @property\r\n    def loadings_x(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            loadings\r\n\r\n        \"\"\"\r\n        return self._P\r\n\r\n    @property\r\n    def weights_y(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            y scores\r\n\r\n        \"\"\"\r\n        return self._C\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pls.py b/pls.py
--- a/pls.py	(revision 8d693ffb6c97112d328f42323843c6f03731e374)
+++ b/pls.py	(date 1718106310441)
@@ -113,3 +113,15 @@
 
         """
         return self._C
+
+    @property
+    def weigths_x(self):
+        """
+
+        Returns
+        -------
+            np.ndarray
+                x weights
+
+        """
+        return self._W
Index: opls.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nOrthogonal Projection on Latent Structure (O-PLS)\r\n\"\"\"\r\nimport numpy as np\r\nfrom numpy import linalg as la\r\nfrom typing import Optional, Tuple\r\nfrom base import nipals\r\n\r\n\r\nclass OPLS:\r\n    \"\"\"\r\n    Orthogonal Projection on Latent Structure (O-PLS).\r\n    Methods\r\n    ----------\r\n    predictive_scores: np.ndarray\r\n        First predictive score.\r\n    predictive_loadings: np.ndarray\r\n        Predictive loadings.\r\n    weights_y: np.ndarray\r\n        y weights.\r\n    orthogonal_loadings: np.ndarray\r\n        Orthogonal loadings.\r\n    orthogonal_scores: np.ndarray\r\n        Orthogonal scores.\r\n    \"\"\"\r\n    def __init__(self):\r\n        \"\"\"\r\n        TODO:\r\n            1. add arg for specifying the method for performing PLS\r\n\r\n        \"\"\"\r\n        # orthogonal score matrix\r\n        self._Tortho: Optional[np.ndarray] = None\r\n        # orthogonal loadings\r\n        self._Portho: Optional[np.ndarray] = None\r\n        # loadings\r\n        self._Wortho: Optional[np.ndarray] = None\r\n        # covariate weights\r\n        self._w: Optional[np.ndarray] = None\r\n\r\n        # predictive scores\r\n        self._T: Optional[np.ndarray] = None\r\n        self._P: Optional[np.ndarray] = None\r\n        self._C: Optional[np.ndarray] = None\r\n        # coefficients\r\n        self.coef: Optional[np.ndarray] = None\r\n        # total number of components\r\n        self.npc: Optional[int] = None\r\n\r\n    def fit(self, x, y, n_comp=None):\r\n        \"\"\"\r\n        Fit PLS model.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        y: np.ndarray\r\n            Dependent matrix with size n samples by 1, or a vector\r\n        n_comp: int\r\n            Number of components, default is None, which indicates that\r\n            largest dimension which is smaller value between n and p\r\n            will be used.\r\n\r\n        Returns\r\n        -------\r\n        OPLS object\r\n\r\n        Reference\r\n        ---------\r\n        [1] Trygg J, Wold S. Projection on Latent Structure (OPLS).\r\n            J Chemometrics. 2002, 16, 119-128.\r\n        [2] Trygg J, Wold S. O2-PLS, a two-block (X-Y) latent variable\r\n            regression (LVR) method with a integral OSC filter.\r\n            J Chemometrics. 2003, 17, 53-64.\r\n\r\n        \"\"\"\r\n        n, p = x.shape\r\n        npc = min(n, p)\r\n        if n_comp is not None and n_comp < npc:\r\n            npc = n_comp\r\n\r\n        # initialization\r\n        Tortho = np.empty((n, npc))\r\n        Portho = np.empty((p, npc))\r\n        Wortho = np.empty((p, npc))\r\n        T, P, C = np.empty((n, npc)), np.empty((p, npc)), np.empty(npc)\r\n\r\n        # X-y variations\r\n        tw = np.dot(y, x) / np.dot(y, y)\r\n        tw /= la.norm(tw)\r\n        # predictive scores\r\n        tp = np.dot(x, tw)\r\n        # components\r\n        w, u, _, t = nipals(x, y)\r\n        p = np.dot(t, x) / np.dot(t, t)\r\n        for nc in range(npc):\r\n            # orthoganol weights\r\n            w_ortho = p - (np.dot(tw, p) * tw)\r\n            w_ortho /= la.norm(w_ortho)\r\n            # orthogonal scores\r\n            t_ortho = np.dot(x, w_ortho)\r\n            # orthogonal loadings\r\n            p_ortho = np.dot(t_ortho, x) / np.dot(t_ortho, t_ortho)\r\n            # update X to the residue matrix\r\n            x -= t_ortho[:, np.newaxis] * p_ortho\r\n            # save to matrix\r\n            Tortho[:, nc] = t_ortho\r\n            Portho[:, nc] = p_ortho\r\n            Wortho[:, nc] = w_ortho\r\n            # predictive scores\r\n            tp -= t_ortho * np.dot(p_ortho, tw)\r\n            T[:, nc] = tp\r\n            C[nc] = np.dot(y, tp) / np.dot(tp, tp)\r\n\r\n            # next component\r\n            w, u, _, t = nipals(x, y)\r\n            p = np.dot(t, x) / np.dot(t, t)\r\n            P[:, nc] = p\r\n\r\n        self._Tortho = Tortho\r\n        self._Portho = Portho\r\n        self._Wortho = Wortho\r\n        # covariate weights\r\n        self._w = tw\r\n\r\n        # coefficients and predictive scores\r\n        self._T = T\r\n        self._P = P\r\n        self._C = C\r\n        self.coef = tw * C[:, np.newaxis]\r\n\r\n        self.npc = npc\r\n\r\n    def predict(self, x, n_component=None, return_scores=False)\\\r\n            -> np.ndarray | Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"\r\n        Predict the new coming data matrix.\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        n_component: int | None\r\n            Number of components.\r\n        return_scores: bool\r\n            Whether the scores should be returned.\r\n\r\n        Returns\r\n        -------\r\n        y: np.ndarray\r\n            Predicted scores for classification.\r\n        score: np.ndarray\r\n            Predictive scores.\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        coef = self.coef[n_component - 1]\r\n\r\n        y = np.dot(x, coef)\r\n        if return_scores:\r\n            return y, np.dot(x, self._w)\r\n\r\n        return y\r\n\r\n    def correct(self, x, n_component=None, return_scores=False, dot=np.dot):\r\n        \"\"\"\r\n        Correction of X\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Data matrix with size n by c, where n is number of\r\n            samples, and c is number of variables\r\n        n_component: int | None\r\n            Number of components. If is None, the number of components\r\n            used in fitting the model is used. Default is None.\r\n        return_scores: bool\r\n            Return orthogonal scores. Default is False.\r\n\r\n        Returns\r\n        -------\r\n        xc: np.ndarray\r\n            Corrected data, with same matrix size with input X.\r\n        t: np.ndarray\r\n            Orthogonal score, n by n_component.\r\n\r\n        \"\"\"\r\n        # TODO: Check X type and dimension consistencies between X and\r\n        #       scores in model.\r\n        xc = x.copy()\r\n        if n_component is None:\r\n            n_component = self.npc\r\n\r\n        if xc.ndim == 1:\r\n            t = np.empty(n_component)\r\n            for nc in range(n_component):\r\n                t_ = dot(xc, self._Wortho[:, nc])\r\n                xc -= t_ * self._Portho[:, nc]\r\n                t[nc] = t_\r\n        else:\r\n            n, c = xc.shape\r\n            t = np.empty((n, n_component))\r\n            # scores\r\n            for nc in range(n_component):\r\n                t_ = dot(xc, self._Wortho[:, nc])\r\n                xc -= t_[:, np.newaxis] * self._Portho[:, nc]\r\n                t[:, nc] = t_\r\n\r\n        if return_scores:\r\n            return xc, t\r\n\r\n        return xc\r\n\r\n    def predictive_score(self, n_component=None):\r\n        \"\"\"\r\n        Parameters\r\n        ----------\r\n        n_component: int\r\n            The component number.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive score.\r\n\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        return self._T[:, n_component-1]\r\n\r\n    def ortho_score(self, n_component=None):\r\n        \"\"\"\r\n\r\n        Parameters\r\n        ----------\r\n        n_component: int\r\n            The component number.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first orthogonal score.\r\n\r\n        \"\"\"\r\n        if n_component is None or n_component > self.npc:\r\n            n_component = self.npc\r\n        return self._Tortho[:, n_component-1]\r\n\r\n    @property\r\n    def predictive_scores(self):\r\n        \"\"\" Orthogonal loadings. \"\"\"\r\n        return self._T\r\n\r\n    @property\r\n    def predictive_loadings(self):\r\n        \"\"\" Predictive loadings. \"\"\"\r\n        return self._P\r\n\r\n    @property\r\n    def weights_y(self):\r\n        \"\"\" y scores. \"\"\"\r\n        return self._C\r\n\r\n    @property\r\n    def orthogonal_loadings(self):\r\n        \"\"\" Orthogonal loadings. \"\"\"\r\n        return self._Portho\r\n\r\n    @property\r\n    def orthogonal_scores(self):\r\n        \"\"\" Orthogonal scores. \"\"\"\r\n        return self._Tortho\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/opls.py b/opls.py
--- a/opls.py	(revision 8d693ffb6c97112d328f42323843c6f03731e374)
+++ b/opls.py	(date 1718106310454)
@@ -42,6 +42,7 @@
         self._T: Optional[np.ndarray] = None
         self._P: Optional[np.ndarray] = None
         self._C: Optional[np.ndarray] = None
+        self._W: Optional[np.ndarray] = None
         # coefficients
         self.coef: Optional[np.ndarray] = None
         # total number of components
@@ -84,7 +85,9 @@
         Tortho = np.empty((n, npc))
         Portho = np.empty((p, npc))
         Wortho = np.empty((p, npc))
-        T, P, C = np.empty((n, npc)), np.empty((p, npc)), np.empty(npc)
+        T = np.empty((n, npc))
+        P = np.empty((p, npc))
+        C = np.empty(npc)
 
         # X-y variations
         tw = np.dot(y, x) / np.dot(y, y)
Index: cross_validation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nPerform cross validation.\r\n\"\"\"\r\nimport collections\r\nimport typing\r\nimport numpy as np\r\nimport numpy.linalg as la\r\n\r\nimport pretreatment\r\nfrom pls import PLS\r\nfrom opls import OPLS\r\n\r\nimport tqdm\r\n\r\n\r\nclass CrossValidation:\r\n    \"\"\"\r\n    Stratified cross validation\r\n\r\n    Parameters:\r\n    ----------\r\n    estimator: str\r\n        Estimator indicates algorithm for model construction.\r\n        Values can be \"pls\" for PLS and \"opls\" for OPLS. Default\r\n        is \"opls\".\r\n    kfold: int\r\n        k fold cross validation. if k equals to len(X), leave one out\r\n        cross validation will be performed. Default is 10.\r\n    scaler: str\r\n        Scaler for scaling data matrix. Valid values are \"uv\" for\r\n        zero-mean-unit-variance scaling, \"pareto\" for Pareto scaling,\r\n        \"minmax\" for Min-Max scaling and \"mean\" for mean centering.\r\n        Default is \"pareto\".\r\n\r\n    Returns\r\n    -------\r\n    CrossValidation object\r\n\r\n    \"\"\"\r\n    def __init__(self, estimator=\"opls\", kfold=10, scaler=\"pareto\"):\r\n        # number of folds\r\n        self.kfold = kfold\r\n        self._scaler_param: str = scaler\r\n        self._estimator_param: str = estimator\r\n        # estimator\r\n        f_estimator, f_scaler = self._create_scaler_estimator()\r\n        self.estimator = f_estimator\r\n        self.scaler = f_scaler\r\n        self.estimator_id = estimator\r\n        # initialize other attributes, but should be HIDDEN\r\n        self._ypred: typing.Optional[np.ndarray] = None\r\n        self._Tortho: typing.Optional[np.ndarray] = None\r\n        self._Tpred: typing.Optional[np.ndarray] = None\r\n        self._ssx: typing.Optional[dict] = None\r\n        self._ssy: typing.Optional[list] = None\r\n        self._pressy: typing.Optional[np.ndarray] = None\r\n        self._n: typing.Optional[int] = None\r\n        self._pcv: typing.Optional[dict] = None\r\n        self._opt_component: typing.Optional[int] = None\r\n        self._mis_classifications: typing.Optional[np.ndarray] = None\r\n        self._q2: typing.Optional[np.ndarray] = None\r\n        self._npc0: typing.Optional[int] = None\r\n        self._x: typing.Optional[np.ndarray] = None\r\n        self.y: typing.Optional[np.ndarray] = None\r\n        self.groups: typing.Optional[dict] = None\r\n        self._used_variable_index: typing.Optional[np.ndarray] = None\r\n        self._r2x_cum: typing.Optional[float] = None\r\n        self._r2y_cum: typing.Optional[float] = None\r\n        self._corr_y_perms: typing.Optional[np.ndarray] = None\r\n        self._perm_q2: typing.Optional[np.ndarray] = None\r\n        self._perm_err: typing.Optional[np.ndarray] = None\r\n\r\n    def fit(self, x, y):\r\n        \"\"\"\r\n        Fitting variable matrix X\r\n\r\n        Parameters\r\n        ----------\r\n        x : np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        y : np.ndarray | list\r\n            Dependent matrix with size n samples by 1. The values in\r\n            this vector must be 0 and 1, otherwise the classification\r\n            performance will be wrongly concluded.\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        # TODO: Check dimension consistencies between X and y.\r\n        # set the labels in y to 0 and 1, and name the groups using\r\n        # the labels in y\r\n        y = self._reset_y(y)\r\n        # matrix dimension\r\n        n, p = x.shape\r\n        # max number of principal components\r\n        npc0 = min(n, p)\r\n        # preallocation\r\n        ssx = collections.defaultdict(lambda: collections.defaultdict(list))\r\n        ssy = []\r\n        ypred, pressy = np.zeros((n, npc0)), np.zeros((n, npc0))\r\n        tortho, tpred = np.zeros((n, npc0)), np.zeros((n, npc0))\r\n        pcv = collections.defaultdict(list)\r\n        for train_index, test_index in self._split(y):\r\n            xtr, xte = x[train_index], x[test_index]\r\n            ytr, yte = y[train_index], y[test_index]\r\n\r\n            # check the data matrix to remove variables having only 1 unique\r\n            # value.\r\n            val_ix, xtr = self._check_x(xtr)\r\n            xte = xte[:, val_ix]\r\n\r\n            # scale matrix\r\n            xtr_scale = self.scaler.fit(xtr)\r\n            xte_scale = self.scaler.scale(xte)\r\n            ytr_scale = self.scaler.fit(ytr)\r\n            yte_scale = self.scaler.scale(yte)\r\n\r\n            # variances\r\n            ssy_tot = (yte_scale ** 2).sum()\r\n            ssx_tot = (xte_scale ** 2).sum()\r\n\r\n            # fit the model\r\n            npc = min(xtr.shape)\r\n            self.estimator.fit(xtr_scale.copy(), ytr_scale, n_comp=npc)\r\n            if npc < npc0:\r\n                npc0 = npc\r\n\r\n            # do prediction iterating through components\r\n            for k in range(1, npc+1):\r\n                # if OPLS is used, the test matrix should be corrected to\r\n                # remove orthogonal components\r\n                if self._estimator_param == \"opls\":\r\n                    xte_corr, tcorr = self.estimator.correct(\r\n                        xte_scale, n_component=k, return_scores=True\r\n                    )\r\n                    # prediction\r\n                    yp_k, tp_k = self.estimator.predict(\r\n                        xte_corr, n_component=k, return_scores=True\r\n                    )\r\n\r\n                    # save the parameters for model quality assessments\r\n                    # Orthogonal and predictive scores\r\n                    if xte_scale.ndim == 1:\r\n                        tortho[test_index, k-1] = tcorr[0]\r\n                    else:\r\n                        tortho[test_index, k-1] = tcorr[:, 0]\r\n                    tpred[test_index, k-1] = tp_k\r\n\r\n                    # sum of squares\r\n                    ssx[k][\"corr\"].append((xte_corr ** 2).sum())\r\n                    xte_ortho = np.dot(\r\n                        tcorr, self.estimator.orthogonal_loadings[:, :k].T\r\n                    )\r\n                    ssx[k][\"xyo\"].append((xte_ortho ** 2).sum())\r\n                    ssx[k][\"total\"].append(ssx_tot)\r\n\r\n                    # covariances from fitting\r\n                    tp = self.estimator.predictive_scores[:, k-1]\r\n                    pcv[k].append(np.dot(tp, xtr_scale) / (tp ** 2).sum())\r\n\r\n                else:\r\n                    # prediction\r\n                    yp_k = self.estimator.predict(xte_scale, n_component=k)\r\n\r\n                # predicted y\r\n                ypred[test_index, k-1] = yp_k\r\n                pressy[test_index, k-1] = (yp_k - yte_scale) ** 2\r\n\r\n            ssy.append(ssy_tot)\r\n\r\n        # save metrics\r\n        self._ypred = ypred[:, :npc0]\r\n        self._pressy = pressy[:, :npc0]\r\n        self._ssy = sum(ssy)\r\n        self._n = n\r\n        self._npc0 = npc0\r\n        self._x = x\r\n        self.y = y\r\n\r\n        # opls specific metrics\r\n        if self._estimator_param == \"opls\":\r\n            self._Tortho = tortho[:, :npc0]\r\n            self._Tpred = tpred[:, :npc0]\r\n            self._ssx = ssx\r\n            self._pcv = pcv\r\n\r\n        # summarize cross validation results\r\n        self._summary_cv()\r\n        # refit for a final model\r\n        self._create_optimal_model(x, y)\r\n\r\n    def predict(self, x, return_scores=False):\r\n        \"\"\"\r\n        Does prediction using optimal model.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Variable matrix with size n samples by p variables.\r\n        return_scores: bool\r\n            For OPLS, it's possible to return predictive scores. Thus\r\n            setting this True with `estimator` being \"opls\" will return\r\n            the predictive scores\r\n\r\n        Returns\r\n        -------\r\n        y: np.ndarray\r\n            Predictions for the x\r\n        scores: np.ndarray\r\n            Predictive scores for OPLS\r\n\r\n        \"\"\"\r\n        # TODO: check the dimension consistencies between the training\r\n        #       data and the input data matrix.\r\n        npc = self._opt_component + 1\r\n        # scale the matrix\r\n        x = self.scaler.scale(x[:, self._used_variable_index])\r\n        if self._estimator_param == \"opls\":\r\n            x = self.estimator.correct(x.copy(), n_component=npc)\r\n            return self.estimator.predict(\r\n                x, n_component=npc, return_scores=return_scores\r\n            )\r\n        return self.estimator.predict(x, n_component=npc)\r\n\r\n    def permutation_test(self, num_perms=10000) -> None:\r\n        \"\"\"\r\n        Performs permutation test on constructed model.\r\n\r\n        Parameters\r\n        ----------\r\n        num_perms: int\r\n            Number of permutations. Defaults to 10000.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n\r\n        \"\"\"\r\n        # check the arguments\r\n        if not isinstance(num_perms, int):\r\n            raise ValueError(\"Expected integer, got {}.\".format(num_perms))\r\n        if num_perms < 20:\r\n            raise ValueError(\"Expected large positive integer >= 20, \"\r\n                             \"got {}.\".format(num_perms))\r\n\r\n        is_opls = self._estimator_param == \"opls\"\r\n\r\n        estimator, scaler = self._create_scaler_estimator()\r\n\r\n        # do permutation test\r\n        x = self._x[:, self._used_variable_index]\r\n        # center y\r\n        y_center = self.y - self.y.mean()\r\n        ssy_c = (y_center ** 2).sum()\r\n        # optimal component number\r\n        npc: int = self._opt_component + 1\r\n        n: int = self.y.size\r\n        pred_y: np.ndarray = np.zeros(n, dtype=np.float64)\r\n\r\n        rnd_generator = np.random.default_rng()\r\n\r\n        perm_q2: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        perm_err: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        perm_corr: np.ndarray = np.zeros(num_perms, dtype=np.float64)\r\n        for i in tqdm.tqdm(range(num_perms), total=num_perms,\r\n                           desc=\"Calculating permuted metrics\"):\r\n            # randomize labels\r\n            ix = rnd_generator.permutation(n)\r\n            rnd_y = self.y[ix]\r\n            ssy: float = 0.\r\n            ssey: float = 0.\r\n            # fit the model using cross validation\r\n            for train_index, test_index in self._split(rnd_y):\r\n                xtr, xte = x[train_index], x[test_index]\r\n                ytr, yte = rnd_y[train_index], rnd_y[test_index]\r\n\r\n                # scale matrix\r\n                xtr_scale = scaler.fit(xtr)\r\n                xte_scale = scaler.scale(xte)\r\n                ytr_scale = scaler.fit(ytr)\r\n                yte_scale = scaler.scale(yte)\r\n\r\n                # variances\r\n                ssy += (yte_scale ** 2).sum()\r\n                # fitting the model\r\n                estimator.fit(xtr_scale.copy(), ytr_scale, n_comp=npc)\r\n                # prediction\r\n                if is_opls:\r\n                    xc = estimator.correct(xte_scale.copy(), n_component=npc)\r\n                    yp = estimator.predict(xc, n_component=npc)\r\n                else:\r\n                    yp = estimator.predict(xte_scale)\r\n                pred_y[test_index] = yp\r\n                ssey += ((yp - yte_scale) ** 2).sum()\r\n\r\n            pred_cls = (pred_y > 0.).astype(int)\r\n            perm_err[i] = np.count_nonzero((pred_cls - rnd_y) != 0) / n\r\n            perm_q2[i] = 1. - ssey / ssy\r\n            perm_corr[i] = abs(((y_center * y_center[ix]).sum()) / ssy_c)\r\n\r\n        self._perm_q2 = perm_q2\r\n        self._perm_err = perm_err\r\n        self._corr_y_perms = perm_corr\r\n\r\n    def reset_optimal_num_component(self, k) -> None:\r\n        \"\"\"\r\n        Resets the optimal number of components for manual setup.\r\n\r\n        Parameters\r\n        ----------\r\n        k: int\r\n            Number of components according to the error plot.\r\n\r\n        Returns\r\n        -------\r\n        None\r\n\r\n        \"\"\"\r\n        if not isinstance(k, int) or k <= 0:\r\n            raise ValueError(\"The number must be a positive integer.\")\r\n\r\n        if k > self._npc0:\r\n            raise ValueError(\"The number must not exceed the maximum \"\r\n                             f\" number of components {self._npc0}.\")\r\n\r\n        self._opt_component = k\r\n        # re-fit the model using the updated optimal number of components\r\n        self._create_optimal_model(self._x, self.y)\r\n\r\n    @property\r\n    def orthogonal_score(self) -> np.ndarray:\r\n        \"\"\"\r\n        Returns cross validated orthogonal score.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first orthogonal scores.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._Tortho[:, self._opt_component]\r\n\r\n    @property\r\n    def predictive_score(self) -> np.ndarray:\r\n        \"\"\"\r\n        Returns cross validated predictive score.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive scores.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._Tpred[:, self._opt_component]\r\n\r\n    @property\r\n    def scores(self) -> np.ndarray:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            The first predictive score, if the method is OPLS/OPLS-DA,\r\n            otherwise is the scores of X\r\n\r\n        \"\"\"\r\n        if self._estimator_param == \"opls\":\r\n            return self.predictive_score\r\n        else:\r\n            return self.estimator.scores_x\r\n\r\n    @property\r\n    def q2(self) -> float:\r\n        \"\"\"\r\n        Returns cross validated Q2.\r\n\r\n        Returns\r\n        -------\r\n        q2: float\r\n\r\n        \"\"\"\r\n        return float(self._q2[self._opt_component])\r\n\r\n    @property\r\n    def optimal_component_num(self) -> int:\r\n        \"\"\"\r\n        Number of components determined by cross validation.\r\n\r\n        Returns\r\n        -------\r\n        int\r\n\r\n        \"\"\"\r\n        return self._opt_component + 1\r\n\r\n    @property\r\n    def R2Xcorr(self) -> float:\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled joint X-y covariation of X.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._r2xcorr[self._opt_component]\r\n\r\n    @property\r\n    def R2XYO(self) -> float:\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled structured noise variation of X.\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._r2xyo[self._opt_component]\r\n\r\n    @property\r\n    def R2X(self) -> float:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled variation of X\r\n\r\n        \"\"\"\r\n        return self._r2x\r\n\r\n    @property\r\n    def R2y(self) -> float:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Modeled variation of y\r\n\r\n        \"\"\"\r\n        return self._r2y\r\n\r\n    @property\r\n    def R2X_cum(self) -> float:\r\n        \"\"\"\r\n        Cumulative fraction of the sum of squares explained up to the\r\n        optimal number of principal components.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Cumulative fraction of the sum of squares explained\r\n\r\n        \"\"\"\r\n        return self._r2x_cum\r\n\r\n    @property\r\n    def R2y_cum(self) -> float:\r\n        \"\"\"\r\n        Cumulative fraction of the sum of squares explained up to the\r\n        optimal number of principal components.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            Cumulative fraction of the sum of squares explained\r\n\r\n        \"\"\"\r\n        return self._r2y_cum\r\n\r\n    @property\r\n    def correlation(self) -> np.ndarray:\r\n        \"\"\" Correlation\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        References\r\n        ----------\r\n        [1] Wiklund S, et al. Visualization of GC/TOF-MS-Based\r\n        Metabolomics Data for Identification of Biochemically\r\n        Interesting Compounds Using OPLS Class Models. Anal Chem.\r\n        2008, 80, 115-122.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._corr\r\n\r\n    @property\r\n    def covariance(self):\r\n        \"\"\"\r\n        Covariance\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        References\r\n        ----------\r\n        [1] Wiklund S, et al. Visualization of GC/TOF-MS-Based\r\n        Metabolomics Data for Identification of Biochemically\r\n        Interesting Compounds Using OPLS Class Models. Anal Chem.\r\n        2008, 80, 115-122.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return self._cov\r\n\r\n    @property\r\n    def loadings_cv(self):\r\n        \"\"\"\r\n        Loadings from cross validation.\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Correlation loading profile\r\n\r\n        Raises\r\n        ------\r\n        ValueError\r\n            If OPLS / OPLS-DA is not used.\r\n\r\n        \"\"\"\r\n        if self._estimator_param != \"opls\":\r\n            raise ValueError(\"This is only applicable for OPLS/OPLS-DA.\")\r\n        return np.array(self._pcv[self._opt_component+1])\r\n\r\n    @property\r\n    def min_nmc(self) -> int:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        int\r\n            Minimal number of mis-classifications obtained by\r\n            cross validation.\r\n\r\n        \"\"\"\r\n        return int(self._mis_classifications[self._opt_component])\r\n\r\n    @property\r\n    def mis_classifications(self) -> np.ndarray:\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        list\r\n            Mis-classifications at different principal components.\r\n\r\n        \"\"\"\r\n        return self._mis_classifications\r\n\r\n    @property\r\n    def used_variable_index(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Indices of variables used for model construction\r\n\r\n        \"\"\"\r\n        return self._used_variable_index\r\n\r\n    @property\r\n    def permutation_q2(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Q2 array generated by permutation test.\r\n\r\n        \"\"\"\r\n        if self._perm_q2 is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._perm_q2\r\n\r\n    @property\r\n    def permutation_error(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Misclassification error rates generated by permutation test.\r\n\r\n        \"\"\"\r\n        if self._perm_err is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._perm_err\r\n\r\n    @property\r\n    def correlation_permute_y(self):\r\n        \"\"\"\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray:\r\n            Correlation between permuted y and normal y.\r\n\r\n        \"\"\"\r\n        if self._corr_y_perms is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        return self._corr_y_perms\r\n\r\n    def p(self, metric=\"q2\"):\r\n        \"\"\"\r\n        Calculates the significance of the constructed model by\r\n        permutation test.\r\n\r\n        Parameters\r\n        ----------\r\n        metric: str\r\n            Metric used to assess the performance of the constructed\r\n            model. \"q2\" and \"error\" are accepted as values.\r\n            \"q2\": Q2\r\n            \"error\": Misclassification error rate.\r\n\r\n        Returns\r\n        -------\r\n        float\r\n            p value\r\n\r\n        \"\"\"\r\n        if self._perm_err is None:\r\n            raise ValueError(\"Permutation test has not been performed.\")\r\n        if metric not in (\"q2\", \"error\"):\r\n            raise ValueError(\"Expected `q2`, `error`, got {}.\".format(metric))\r\n\r\n        if metric == \"q2\":\r\n            nb: int = np.count_nonzero(self._perm_q2 >= self.q2) + 1\r\n            nt: float = self._perm_q2.size + 1.\r\n        else:\r\n            err: float = self.min_nmc / self.y.size\r\n            nb: int = np.count_nonzero(self._perm_err <= err) + 1\r\n            nt: float = self._perm_err.size + 1.\r\n\r\n        return nb / nt\r\n\r\n    @staticmethod\r\n    def _check_x(x) -> typing.Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"\r\n        Checks the valid variables to remove those with unique value.\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            Data matrix\r\n\r\n        Returns\r\n        -------\r\n        index: np.ndarray\r\n            Indices of valid variables\r\n        x: np.ndarray\r\n            Valid data matrix\r\n\r\n        \"\"\"\r\n        # check nan and inf\r\n        has_nan = np.isnan(x).any(axis=0)\r\n        has_inf = np.isinf(x).any(axis=0)\r\n        idx, = np.where(~(has_nan | has_inf))\r\n        # check unique value\r\n        is_unique_value = np.absolute(\r\n            x[:, idx] - x[:, idx].mean(axis=0)\r\n        ).sum(axis=0) == 0\r\n        # index of valid variables\r\n        idx = idx[~is_unique_value]\r\n\r\n        return idx, x[:, idx]\r\n\r\n    def _split(self, y) -> typing.Iterable:\r\n        \"\"\"\r\n        Split total number of n samples into training and testing data.\r\n\r\n        Parameters\r\n        ----------\r\n        y: np.ndarray\r\n            Number of samples\r\n\r\n        Returns\r\n        -------\r\n        iterator\r\n\r\n        \"\"\"\r\n        n, k = y.size, self.kfold\r\n        groups, counts = np.unique(y, return_counts=True)\r\n\r\n        # check the number\r\n        if counts.min() < k and n != k:\r\n            raise ValueError(f\"The fold number {k} is larger than the least\"\r\n                             f\" group number {counts.min()}.\")\r\n\r\n        indices = np.arange(n, dtype=int)\r\n        # leave one out cross validation\r\n        if n == k:\r\n            for i in indices:\r\n                yield np.delete(indices, i), [i]\r\n\r\n        # k fold cross validation\r\n        else:\r\n            group_index, blks = [], []\r\n            for g, nk in zip(groups, counts):\r\n                group_index.append(np.where(y == g)[0])\r\n                blks.append(nk // k if nk % k == 0 else nk // k + 1)\r\n            # splitting data\r\n            for i in range(k):\r\n                trains = np.ones(n, dtype=bool)\r\n                for blk, idx, nk in zip(blks, group_index, counts):\r\n                    trains[idx[blk * i: min(blk * (i + 1), nk)]] = False\r\n                yield indices[trains], indices[np.logical_not(trains)]\r\n\r\n    def _create_optimal_model(self, x, y) -> None:\r\n        \"\"\"\r\n        Create final model based on the optimal number of components.\r\n        \"\"\"\r\n        val_ix, x = self._check_x(x)\r\n\r\n        # scale data matrix\r\n        y_scale = self.scaler.fit(y)\r\n        x_scale = self.scaler.fit(x)\r\n\r\n        # optimal component number\r\n        npc = self._opt_component+1\r\n\r\n        # fit the model\r\n        self.estimator.fit(x_scale.copy(), y_scale.copy(), n_comp=npc)\r\n\r\n        # summary the fitting\r\n        self._summary_fit(x_scale, y_scale)\r\n\r\n        # indices of variables used for model construction\r\n        self._used_variable_index = val_ix\r\n\r\n    def _summary_fit(self, x, y) -> None:\r\n        \"\"\"\r\n\r\n        Parameters\r\n        ----------\r\n        x: np.ndarray\r\n            scaled variable matrix.\r\n        y: np.ndarray\r\n            scaled dependent variable\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        npc = self._opt_component + 1\r\n        # Calculate covariance and correlation for variable importance\r\n        # assessment. Only works for OPLS/OPLS-DA\r\n        r2x_pc: np.ndarray = np.zeros(npc, dtype=np.float64)\r\n        r2y_pc: np.ndarray = np.zeros(npc, dtype=np.float64)\r\n        ssx: float = (x ** 2).sum()\r\n        ssy: float = (y ** 2).sum()\r\n        if self._estimator_param == \"opls\":\r\n            tp = self.estimator.predictive_score(npc)\r\n            ss_tp = np.dot(tp, tp)\r\n            # loadings\r\n            w = np.dot(tp, x)\r\n            self._cov = w / ss_tp\r\n            self._corr = w / (np.sqrt(ss_tp) * la.norm(x, axis=0))\r\n\r\n            # reconstruct variable matrix X\r\n            # from orthogonal corrections.\r\n            o_scores = self.estimator.orthogonal_scores\r\n            o_loads = self.estimator.orthogonal_loadings\r\n            p_scores = self.estimator.predictive_scores\r\n            p_loads = self.estimator.predictive_loadings\r\n            for i in range(npc):\r\n                xrec = np.dot(o_scores[:, i][:, np.newaxis],\r\n                              o_loads[:, i][np.newaxis, :])\r\n                # from predictive scores\r\n                xrec += np.dot(p_scores[:, i][:, np.newaxis],\r\n                               p_loads[:, i][np.newaxis, :])\r\n                r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx\r\n\r\n                # reconstruct dependent vector y\r\n                yrec = p_scores[:, i] * self.estimator.weights_y[i]\r\n                r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy\r\n        else:\r\n            for i in range(npc):\r\n                xrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],\r\n                              self.estimator.loadings_x[:, i][:, np.newaxis])\r\n                yrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],\r\n                              self.estimator.weights_y[i])\r\n                r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx\r\n                r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy\r\n\r\n        # r2x\r\n        self._r2x = 1. - r2x_pc[self._opt_component]\r\n        # r2y\r\n        self._r2y = 1. - r2y_pc[self._opt_component]\r\n        # cumulative r2x\r\n        self._r2x_cum = 1. - np.prod(r2x_pc)\r\n        # cumulative r2y\r\n        self._r2y_cum = 1. - np.prod(r2y_pc)\r\n\r\n    def _summary_cv(self) -> None:\r\n        \"\"\"\r\n        Summary cross validation results to calculate metrics for\r\n        assessing the model.\r\n\r\n        Returns\r\n        -------\r\n        CrossValidation object\r\n\r\n        \"\"\"\r\n        # number of mis-classifications\r\n        _pred_class = (self._ypred > 0).astype(float)\r\n        nmc = ((_pred_class - self.y[:, np.newaxis]) != 0).sum(axis=0)\r\n        j = int(np.argmin(nmc))\r\n        # optimal number of components\r\n        self._opt_component: int = j\r\n        self._mis_classifications = nmc\r\n        # Q2\r\n        self._q2 = 1. - self._pressy.sum(axis=0) / self._ssy\r\n        # metrics for OPLS\r\n        if self._estimator_param == \"opls\":\r\n            _, npc = _pred_class.shape\r\n            # r2xcorr, r2xyo\r\n            r2xcorr, r2xyo = [], []\r\n            for k in range(1, npc+1):\r\n                r2xcorr.append(\r\n                    sum(self._ssx[k][\"corr\"]) / sum(self._ssx[k][\"total\"])\r\n                )\r\n                r2xyo.append(\r\n                    sum(self._ssx[k][\"xyo\"]) / sum(self._ssx[k][\"total\"])\r\n                )\r\n            self._r2xcorr = r2xcorr\r\n            self._r2xyo = r2xyo\r\n\r\n    def _reset_y(self, y) -> np.ndarray:\r\n        \"\"\"\r\n        Reset the labels in y to 0 and 1, and name the groups using the\r\n        labels in y.\r\n\r\n        Parameters\r\n        ----------\r\n        y: np.ndarray | list\r\n\r\n        Returns\r\n        -------\r\n        np.ndarray\r\n            Label reset in y.\r\n\r\n        \"\"\"\r\n        if isinstance(y, list):\r\n            y = np.array([str(v) for v in y], dtype=str)\r\n\r\n        # groups\r\n        labels = np.unique(y)\r\n        # only binary classification is allowed.\r\n        if labels.size != 2:\r\n            raise ValueError(\r\n                \"Only binary classification is currently accepted.\"\r\n            )\r\n\r\n        # reset the values for each class\r\n        groups = collections.defaultdict()\r\n        y_reset = np.zeros_like(y, dtype=float)\r\n        for i, label in enumerate(labels):\r\n            y_reset[y == label] = i\r\n            groups[i] = label if isinstance(label, str) else str(int(label))\r\n\r\n        self.groups = groups\r\n        return y_reset\r\n\r\n    def _create_scaler_estimator(self):\r\n        \"\"\"\r\n        Creates scaler and estimator.\r\n\r\n        Returns\r\n        -------\r\n\r\n        \"\"\"\r\n        if self._estimator_param == \"pls\":\r\n            return PLS(), pretreatment.Scaler(scaler=self._scaler_param)\r\n        if self._estimator_param == \"opls\":\r\n            return OPLS(), pretreatment.Scaler(scaler=self._scaler_param)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cross_validation.py b/cross_validation.py
--- a/cross_validation.py	(revision 8d693ffb6c97112d328f42323843c6f03731e374)
+++ b/cross_validation.py	(date 1718106310473)
@@ -69,6 +69,7 @@
         self._corr_y_perms: typing.Optional[np.ndarray] = None
         self._perm_q2: typing.Optional[np.ndarray] = None
         self._perm_err: typing.Optional[np.ndarray] = None
+        self._vip: typing.Optional[np.ndarray] = None
 
     def fit(self, x, y):
         """
@@ -754,6 +755,52 @@
                     trains[idx[blk * i: min(blk * (i + 1), nk)]] = False
                 yield indices[trains], indices[np.logical_not(trains)]
 
+    def _cal_vip(self) -> None:
+        """
+        Calculates variable importance in projection (VIP).
+        """
+        npc = self._opt_component + 1
+        p = self._x.shape[1]
+        w_weights: np.ndarray = np.zeros((npc, p), dtype=np.float64)
+        if self._scaler_param == "uv":
+            # already standardized to zero mean and unit variance,
+            # directly use the results
+            if self._estimator_param == "opls":
+                tp = self.estimator.predictive_score(npc)
+                ss_tp = np.dot(tp, tp)
+                # loadings
+                w = np.dot(tp, x)
+                self._cov = w / ss_tp
+                self._corr = w / (np.sqrt(ss_tp) * la.norm(x, axis=0))
+
+                # reconstruct variable matrix X
+                # from orthogonal corrections.
+                o_scores = self.estimator.orthogonal_scores
+                o_loads = self.estimator.orthogonal_loadings
+                p_scores = self.estimator.predictive_scores
+                p_loads = self.estimator.predictive_loadings
+                for i in range(npc):
+                    xrec = np.dot(o_scores[:, i][:, np.newaxis],
+                                  o_loads[:, i][np.newaxis, :])
+                    # from predictive scores
+                    xrec += np.dot(p_scores[:, i][:, np.newaxis],
+                                   p_loads[:, i][np.newaxis, :])
+                    r2x_pc[i] = ((x - xrec) ** 2).sum() / ssx
+
+                    # reconstruct dependent vector y
+                    yrec = p_scores[:, i] * self.estimator.weights_y[i]
+                    r2y_pc[i] = ((y - yrec) ** 2).sum() / ssy
+            else:
+                ssy_exp: float = 0.
+                for i in range(npc):
+                    yrec = np.dot(self.estimator.scores_x[:, i][:, np.newaxis],
+                                  self.estimator.weights_y[i])
+                    ssk = (yrec ** 2).sum()
+                    ssy_exp += ssk
+                    w_weights[i] = (self.estimator.weights_x[:, i] ** 2) * ssk
+                vips = np.sqrt(w_weights.sum(axis=0) * p / ssy_exp)
+        self._vip = vips.copy()
+
     def _create_optimal_model(self, x, y) -> None:
         """
         Create final model based on the optimal number of components.
